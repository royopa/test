# -*- coding: utf-8 -*-
"""
Created on Thu Sep 12 11:04:59 2019

@author: marcgut
"""

import sys
import pandas as pd
import numpy as np
sys.path.append(r'F:\SISTDAD\MEMPGRP\BMORIER\python\lib')

from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.preprocessing import StandardScaler


y = np.array([ -0.3,   3.3,   3. ,   1.2,   3.8,   3.5,   0.4,   3.3,   2.4,
         4. ,   4.9,   4.5,   5.8,   6. ,   6.3,   5.2,  10.3,   6.5,
         5.6,   7.8,   8.5,   1.3,   8.6,   2.6,   6.4,   8.7,   2. ,
         8.9,   0.6,  -6.2, -13.5, -16.1, -15. ,  -9.1, -14. , -11.2,
       -11.1, -10.8,  -7.3,  -7.9,  -2.8,   5.2,  15. ,  12.5,  12.9,
        16.5,  14.1,  13. ,  10.3,   9.2,   8.5,   6.4,   2.1,   5.4,
         2.5,   2. ,   6.4,  -0.7,  -1.8,   2.8,   0.3,  -0.8,   2.7,
        -1. ,  -1.4,  -2.6,  -0.9,  -4.5,  -5.6,  -4.7,  -4.7,  -4.6,
        -4.5,  -1.6,   0.7,  -1.4,   5.5,   0.6,  -3.5,   5.8,  -1.7,
        -2. ,   9. ,   2.5,   3.4,   3.5,   0.5,   3.9,   0.8,   1.3,
        -2.1,  -1.9,   4.2,  -0.4,  -5.8,  -3.3,  -6.8,  -3.6,  -5.7,
        -1.7,  -3.3,  -6.3,  -2.4,  -4.5,  -8.6,  -3. ,  -7.2,  -8.6,
        -2.4,  -8.9,  -8.7, -11.3, -12.1, -12.2, -10.5, -11.8,  -7.9,
       -10.6,  -5.8,  -6.8,  -4.8,  -5.9,  -4.6,  -3.6,  -7. ,  -1.1,
         0. ,   1.6,   0.1,   1.7,  -3.7,   3.8,   0.7,   2.5,   3.7,
         2.3,   4.9,   4.1,   3.8,   4.4,   1.4,   0.9,   7.2,  -5.7,
         2.7,   3.7,   1.5,  -2.3,   0.7,  -1.1,  -3. ,  -1.8,   1.7,
        -5.3,  -3.2,   6.4])

X = np.array([[ 4.30000e+00, -8.35200e+03],
       [-3.00000e-01,  2.98400e+03],
       [ 3.30000e+00,  1.08820e+04],
       [ 3.00000e+00, -1.02550e+04],
       [ 1.20000e+00,  2.46940e+04],
       [ 3.80000e+00,  2.00300e+03],
       [ 3.50000e+00, -1.68720e+04],
       [ 4.00000e-01,  7.13800e+03],
       [ 3.30000e+00,  4.19000e+02],
       [ 2.40000e+00,  2.21570e+04],
       [ 4.00000e+00,  2.76150e+04],
       [ 4.90000e+00,  2.25010e+04],
       [ 4.50000e+00,  2.74210e+04],
       [ 5.80000e+00,  5.01950e+04],
       [ 6.00000e+00,  4.20810e+04],
       [ 6.30000e+00,  5.31000e+04],
       [ 5.20000e+00,  7.57490e+04],
       [ 1.03000e+01,  5.38320e+04],
       [ 6.50000e+00,  3.93360e+04],
       [ 5.60000e+00,  5.16780e+04],
       [ 7.80000e+00,  4.86960e+04],
       [ 8.50000e+00,  3.33340e+04],
       [ 1.30000e+00,  7.09180e+04],
       [ 8.60000e+00,  3.26980e+04],
       [ 2.60000e+00,  5.95860e+04],
       [ 6.40000e+00,  5.22460e+04],
       [ 8.70000e+00,  3.56320e+04],
       [ 2.00000e+00,  4.49100e+04],
       [ 8.90000e+00, -3.22700e+03],
       [ 6.00000e-01, -7.53550e+04],
       [-6.20000e+00, -1.25711e+05],
       [-1.35000e+01, -6.22140e+04],
       [-1.61000e+01, -4.26680e+04],
       [-1.50000e+01, -2.12300e+03],
       [-9.10000e+00, -3.85940e+04],
       [-1.40000e+01, -1.20900e+04],
       [-1.12000e+01, -1.59770e+04],
       [-1.11000e+01, -3.39990e+04],
       [-1.08000e+01, -1.46520e+04],
       [-7.30000e+00, -2.28430e+04],
       [-7.90000e+00,  2.61130e+04],
       [-2.80000e+00,  9.51380e+04],
       [ 5.20000e+00,  1.49280e+05],
       [ 1.50000e+01,  5.31530e+04],
       [ 1.25000e+01,  4.00680e+04],
       [ 1.29000e+01,  5.18490e+04],
       [ 1.65000e+01,  3.01730e+04],
       [ 1.41000e+01,  3.49220e+04],
       [ 1.30000e+01,  7.55200e+03],
       [ 1.03000e+01,  1.93710e+04],
       [ 9.20000e+00,  2.34830e+04],
       [ 8.50000e+00,  1.46740e+04],
       [ 6.40000e+00, -1.27240e+04],
       [ 2.10000e+00,  1.94280e+04],
       [ 5.40000e+00,  2.41860e+04],
       [ 2.50000e+00,  5.90600e+03],
       [ 2.00000e+00,  5.93320e+04],
       [ 6.40000e+00, -2.16890e+04],
       [-7.00000e-01,  8.34600e+03],
       [-1.80000e+00,  7.87700e+03],
       [ 2.80000e+00,  1.20530e+04],
       [ 3.00000e-01,  1.80420e+04],
       [-8.00000e-01,  1.78260e+04],
       [ 2.70000e+00, -1.73110e+04],
       [-1.00000e+00, -2.26180e+04],
       [-1.40000e+00, -2.85820e+04],
       [-2.60000e+00, -3.53500e+03],
       [-9.00000e-01, -2.86970e+04],
       [-4.50000e+00, -7.68780e+04],
       [-5.60000e+00,  1.68560e+04],
       [-4.70000e+00, -1.69440e+04],
       [-4.70000e+00, -1.90360e+04],
       [-4.60000e+00, -1.83280e+04],
       [-4.50000e+00, -7.65600e+03],
       [-1.60000e+00,  6.97900e+03],
       [ 7.00000e-01,  2.69190e+04],
       [-1.40000e+00,  5.35340e+04],
       [ 5.50000e+00,  4.13620e+04],
       [ 6.00000e-01,  7.07000e+03],
       [-3.50000e+00,  8.16430e+04],
       [ 5.80000e+00,  2.15340e+04],
       [-1.70000e+00,  1.57540e+04],
       [-2.00000e+00,  8.62560e+04],
       [ 9.00000e+00,  5.83400e+04],
       [ 2.50000e+00,  4.61310e+04],
       [ 3.40000e+00,  1.55970e+04],
       [ 3.50000e+00,  1.03700e+04],
       [ 5.00000e-01,  3.47270e+04],
       [ 3.90000e+00, -1.80600e+03],
       [ 8.00000e-01, -2.15650e+04],
       [ 1.30000e+00, -3.72080e+04],
       [-2.10000e+00, -5.49590e+04],
       [-1.90000e+00,  4.10250e+04],
       [ 4.20000e+00, -5.61780e+04],
       [-4.00000e-01, -7.54400e+04],
       [-5.80000e+00, -6.32000e+04],
       [-3.30000e+00, -1.07919e+05],
       [-6.80000e+00, -6.40210e+04],
       [-3.60000e+00, -7.89360e+04],
       [-5.70000e+00, -2.19700e+04],
       [-1.70000e+00, -2.93490e+04],
       [-3.30000e+00, -2.83250e+04],
       [-6.30000e+00, -2.72700e+04],
       [-2.40000e+00, -3.25830e+04],
       [-4.50000e+00, -7.56770e+04],
       [-8.60000e+00, -1.76880e+04],
       [-3.00000e+00, -5.74690e+04],
       [-7.20000e+00, -6.83120e+04],
       [-8.60000e+00, -2.87490e+04],
       [-2.40000e+00, -2.95780e+04],
       [-8.90000e+00, -4.76570e+04],
       [-8.70000e+00, -1.26995e+05],
       [-1.13000e+01, -8.94600e+04],
       [-1.21000e+01, -9.09660e+04],
       [-1.22000e+01, -6.20390e+04],
       [-1.05000e+01, -5.19270e+04],
       [-1.18000e+01, -6.16170e+04],
       [-7.90000e+00, -5.47140e+04],
       [-1.06000e+01, -4.83240e+04],
       [-5.80000e+00, -3.41480e+04],
       [-6.80000e+00, -9.35000e+02],
       [-4.80000e+00, -3.13610e+04],
       [-5.90000e+00, -3.57720e+04],
       [-4.60000e+00, -1.70600e+03],
       [-3.60000e+00, -2.62980e+04],
       [-7.00000e+00,  4.52200e+04],
       [-1.10000e+00,  5.93450e+04],
       [ 0.00000e+00,  2.65980e+04],
       [ 1.60000e+00,  5.74300e+04],
       [ 1.00000e-01,  3.62160e+04],
       [ 1.70000e+00,  2.02560e+04],
       [-3.70000e+00,  7.45030e+04],
       [ 3.80000e+00,  2.85880e+04],
       [ 7.00000e-01,  3.35980e+04],
       [ 2.50000e+00,  8.52920e+04],
       [ 3.70000e+00,  6.76580e+04],
       [ 2.30000e+00,  7.63360e+04],
       [ 4.90000e+00,  3.49890e+04],
       [ 4.10000e+00,  1.85540e+04],
       [ 3.80000e+00,  3.93410e+04],
       [ 4.40000e+00,  1.17350e+04],
       [ 1.40000e+00,  3.09710e+04],
       [ 9.00000e-01,  7.45860e+04],
       [ 7.20000e+00, -4.10690e+04],
       [-5.70000e+00,  4.14670e+04],
       [ 2.70000e+00,  2.02640e+04],
       [ 3.70000e+00,  2.46100e+04],
       [ 1.50000e+00, -1.66820e+04],
       [-2.30000e+00,  9.28000e+03],
       [ 7.00000e-01, -9.30200e+03],
       [-1.10000e+00, -4.21940e+04],
       [-3.00000e+00, -1.95720e+04],
       [-1.80000e+00,  4.44410e+04],
       [ 1.70000e+00, -2.67760e+04],
       [-5.30000e+00,  1.42100e+03],
       [-3.20000e+00,  6.34180e+04]])




X_i = np.split(ary=X, indices_or_sections=X.shape[1], axis=1)
X_dic = {}
for i, el in enumerate(X_i):
    X_dic[str(i)] = el


# Test 1: separate models

model_1 = make_pipeline(StandardScaler(), LinearRegression(fit_intercept=False,
                        normalize=False))

model_2 = make_pipeline(StandardScaler(), LinearRegression(fit_intercept=False,
                        normalize=False))

model_1.fit(X_dic['0'], y)
model_2.fit(X_dic['1'], y)

pred_1_1 = model_1.predict(X_dic['0'])
pred_1_2 = model_2.predict(X_dic['1'])


# Test 2: in loop with dictionary
model_dic = {}
for key in X_dic:
    model_dic[key] = make_pipeline(StandardScaler(), LinearRegression(fit_intercept=False,
                        normalize=False)).fit(X_dic[key], y)

preds = {}   
for key in model_dic:
    model_i = model_dic[key]
    preds[key] = model_i.predict(X_dic[key])





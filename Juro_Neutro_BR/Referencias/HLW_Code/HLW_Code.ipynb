{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HLW Replication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook documents the R code used for the estimation of the natural rate of interest, potential GDP, and its trend growth rate for the United States, Canada, the Euro Area, and the UK, presented in \"Measuring the Natural Rate of Interest: International Trends and Determinants\" (Holston, Laubach, Williams 2017; henceforth HLW). It also catalogues steps to download and prepare data used in the estimation. The code documented here reflects the final December 2016 version of the paper, with an estimation sample of 1961:Q1 through 2016:Q3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Layout and Directory Structure\n",
    "There is one main R file, *run.hlw.R*, which does the following:  \n",
    "    1. Prepares data to be used in the HLW estimation;  \n",
    "    2. Runs the three-stage HLW estimation for each economy;   \n",
    "    3. Saves output.\n",
    "    \n",
    "**Please note that running *run.hlw.R* and running this notebook are equivalent. This notebook walks the user through *run.hlw.R* and the functions that it calls. All code is included inline in this notebook; no R files are sourced or required.** \n",
    "    \n",
    "This file calls multiple R functions and files, each of which are described and included in this guide. To run the code without modification, use the following structure:  \n",
    "    1. A subdirectory titled \"rawData\" should contain all data described below, downloaded and saved as CSV files;  \n",
    "    2. An empty subdirectory titled \"inputData\" should be created and will populate with prepared data;  \n",
    "    3. An empty subdirectory titled \"output\" should be created and will populate with estimates and model output.\n",
    "    4. Optional: create a subdirectory titled \"Rpackages\" to use as the library location for downloaded packages.\n",
    "    \n",
    "**For the US, raw data will be downloaded automatically. For all other economies, the user must manually download the data and save it in the rawData subdirectory.**\n",
    "\n",
    "R Version: 3.2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data\n",
    "A further description of the data we require can be found in the Data Appendix to the paper. For each economy, we require data for real GDP, inflation, and the short-term nominal interest rate, as well as a procedure to compute inflation expectations to calculate the ex ante real short-term interest rate. The inflation measure is the annualized quarterly growth rate of the specified consumer price series. Interest rates are expressed on a 365-day annualized basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### United States\n",
    "We use real GDP and core PCE data published by the Bureau of Economic Analysis. The short-term interest rate is the annualized nominal federal funds rate, available from the Board of Governors. Because the federal funds rate frequently fell below the discount rate prior to 1965, we use the Federal Reserve Bank of New York's discount rate, part of the IMF's International Financial Statistics Yearbooks (IFS), prior to 1965. All US data can be downloaded from the St. Louis Fed's Federal Reserve Economic Data (FRED) website, and will be automatically downloaded in *prepare.rstar.data.us.R.*\n",
    "#### FRED Mnemonics:   \n",
    "Real GDP: GDPC1  \n",
    "Core PCE: PCEPILFE  \n",
    "Federal Funds Rate: FEDFUNDS  \n",
    "FRBNY Discount Rate: INTDSRUSM193N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canada\n",
    "We use real GDP from the IMF's IFS. The short-term nominal interest rate is the Bank of Canada's target for the overnight rate, taken as the end-of-period value for each month and aggregated to quarterly frequency. Prior to May 2001, we use the Bank of Canada's bank rate as the short-term interest rate. We use the BoC's core consumer price index to construct our inflation series. Prior to 1984, we use CPI containing all items. With the exception of GDP, all data is from Statistics Canada.\n",
    "\n",
    "#### Mnemonics:\n",
    "Real GDP: IFS series \"Gross Domestic Product, Real, Seasonally adjusted, Index\"  \n",
    "Core CPI: v41690926 (Table 326-0022); Source: Statistics Canada  \n",
    "CPI: v41690914 (Table 326-0022); v41690973 (Table 326-0020); Source: Statistics Canada  \n",
    "Bank Rate: v122530 (Table 176-0043); Source: Statistics Canada  \n",
    "Target Rate: v39079 (Table 176-0048); Source: Statistics Canada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euro Area\n",
    "All data is from the Area Wide Model (AWM), which is available from the Euro Area Business Cycle Network. We use the core price index beginning in 1988 and the total price index prior; because data availability is longer for the non-seasonally adjusted series, we use those and seasonally adjust them. The nominal short-term interest rate is the three-month rate. Since the AWM is released annually, we update each series using data from the ECB's Statistical Data Warehouse. For example, at the time of publication we used AWM data through 2015:Q4 and updated the series using ECB data for the first three quarters of 2016.\n",
    "\n",
    "#### AWM Mnemonics:  \n",
    "Real GDP: YER  \n",
    "Price Index: HICP (not seasonally adjusted)  \n",
    "Core Price Index: HEX  \n",
    "Nominal Short-term Rate: STN\n",
    "\n",
    "#### ECB Statistical Data Warehouse Mnemonics:  \n",
    "Real GDP: MNA.Q.Y.I8.W2.S1.S1.B.B1GQ.\\_Z.\\_Z.\\_Z.EUR.LR.N  \n",
    "Core Price Index: ICP.M.U2.N.XE0000.4.INX  \n",
    "Nominal Short-term Rate: FM.Q.U2.EUR.RT.MM.EURIBOR3MD\\_.HSTA  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### United Kingdom\n",
    "GDP data are taken from the Office of National Statistics (ONS). Our inflation measure is core CPI; prior to 1970 we use all-items CPI; both are from the OECD. The short-term interest rate is the Bank of England's Official Bank Rate.\n",
    "\n",
    "#### Mnemonics:  \n",
    "Real GDP: Series ABMI; Source: ONS  \n",
    "CPI: Series \"MEI Prices: Consumer prices - all items\"; Source: OECD  \n",
    "Core CPI: Series \"MEI Prices: Consumer prices - all items non-food, non-energy\"; Source: OECD  \n",
    "Nominal Short-term Rate: \"Official Bank Rate History\"; Source: Bank of England Statistical Interactive Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Functions used Throughout HLW Programs\n",
    "In the accompanying set of code, these functions are stored in *utilities.R*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function:**     *shiftQuarter*  \n",
    "**Description:**  This function takes in a (year, quarter) date in time series format and a shift number, and returns the (year, quarter) date corresponding to the shift. Positive values of shift produce leads and negative values of shift produce lags. For example, entering 2014q1 with a shift of -1 would return 2013q4. Entering 2014q1 with a shift of 1 would return 2014q2. In each case, the first argument of the function must be entered as a two-element vector, where the first element corresponds to the year and the second element corresponds to the quarter. For example, 2014q1 must be entered as \"c(2014, 1)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shiftQuarter <- function(original.start,shift){\n",
    "# Leads (positive values of shift)\n",
    "    if (shift > 0) {\n",
    "        new.start = c(0,0)\n",
    "        sum = original.start[2] + shift\n",
    "    \n",
    "        # Get the year value\n",
    "        if (sum <= 4) {\n",
    "            new.start[1] = original.start[1]\n",
    "        }\n",
    "        else {\n",
    "            new.start[1] = original.start[1] + ceiling(sum/4) - 1\n",
    "        }\n",
    "\n",
    "        # Get the quarter value\n",
    "        if (sum %% 4 > 0) {\n",
    "            new.start[2] = sum %% 4\n",
    "        }\n",
    "        else {\n",
    "            new.start[2] = sum %% 4 + 4\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Lags (negative values of shift)\n",
    "    else {\n",
    "        new.start = c(0,0)\n",
    "        diff = original.start[2] - abs(shift)\n",
    "    \n",
    "        # Get the year value\n",
    "        if (diff > 0) {\n",
    "            new.start[1] = original.start[1]\n",
    "        }\n",
    "        else {\n",
    "            new.start[1] = original.start[1] - (1 + floor(abs(diff)/4))\n",
    "        }\n",
    "\n",
    "        # Get the quarter value\n",
    "        if (diff %% 4 > 0) {\n",
    "            new.start[2] = diff %% 4\n",
    "        }\n",
    "        else {\n",
    "            new.start[2] = diff %% 4 + 4\n",
    "        }\n",
    "    }\n",
    "        \n",
    "return(new.start)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function**: *shiftMonth*  \n",
    "**Description**: This function takes in a (year, month) date in time series format and a shift number, and returns the (year, month) date corresponding to the shift. Positive values of shift produce leads and negative values of shift produce lags. For example, entering 2014m1 with a shift of -1 would return 2013m12. Entering 2014m1 with a shift of 1 would return 2014m2. In each case, the first argument of the function must be entered as a two-element vector, where the first element corresponds to the year and the second element corresponds to the month. This function is analogous to shiftQuarter()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shiftMonth <- function(original.start,shift){\n",
    "# Leads (positive values of shift)\n",
    "    if (shift > 0) {\n",
    "        new.start = c(0,0)\n",
    "        sum = original.start[2] + shift\n",
    "    \n",
    "        # Get the year value\n",
    "        if (sum <= 12) {\n",
    "            new.start[1] = original.start[1]\n",
    "        }\n",
    "        else {\n",
    "            new.start[1] = original.start[1] + ceiling(sum/12) - 1\n",
    "        }\n",
    "\n",
    "        # Get the month value\n",
    "        if (sum %% 12 > 0) {\n",
    "            new.start[2] = sum %% 12\n",
    "        }\n",
    "        else {\n",
    "            new.start[2] = sum %% 12 + 12\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Lags (negative values of shift)\n",
    "    else {\n",
    "        new.start = c(0,0)\n",
    "        diff = original.start[2] - abs(shift)\n",
    "    \n",
    "        # Get the year value\n",
    "        if (diff > 0) {\n",
    "            new.start[1] = original.start[1]\n",
    "        }\n",
    "        else {\n",
    "            new.start[1] = original.start[1] - (1 + floor(abs(diff)/12))\n",
    "        }\n",
    "\n",
    "        # Get the month value\n",
    "        if (diff %% 12 > 0) {\n",
    "            new.start[2] = diff %% 12\n",
    "        }\n",
    "        else {\n",
    "            new.start[2] = diff %% 12 + 12\n",
    "        }\n",
    "    }\n",
    "        \n",
    "return(new.start)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function**: *getFRED*  \n",
    "**Description**: This function downloads data from FRED. It returns quarterly data. User must provide the FRED url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "getFRED <- function(url, freq = \"Quarterly\") {\n",
    "    # Download the data from FRED\n",
    "    \n",
    "    #download.file(url, destfile = 'FREDtemp.txt', method = \"wget\")\n",
    "    #FREDraw <- readLines('FREDtemp.txt')\n",
    "    \n",
    "    txt.file.name <- paste0(\"rawData/\",substr(url, regexpr('[a-zA-z0-9]*.txt',url),1000))\n",
    "    if (!file.exists(txt.file.name)){\n",
    "        # Download the data from FRED\n",
    "        #download.file(url, destfile = 'FREDtemp.txt', method = \"wget\")\n",
    "        system(paste0('wget --no-check-certificate \"', url, '\"'))\n",
    "        system(paste('mv',substr(url, regexpr('[a-zA-z0-9]*.txt',url),1000),txt.file.name))\n",
    "    }\n",
    "    FREDraw <- readLines(txt.file.name) \n",
    "\n",
    "    # Frequency\n",
    "    freq.FRED <- gsub(' ', '',substr(FREDraw[which(regexpr('Frequency', FREDraw)==1)],\n",
    "                                     (nchar('Frequency')+2),100))    \n",
    "\n",
    "    # Where does the data start\n",
    "    datastart = which(gsub(' ', '',FREDraw)=='DATEVALUE') - 2\n",
    "\n",
    "    #data <- read.table('FREDtemp.txt', skip = datastart, header = TRUE)\n",
    "    data <- read.table(txt.file.name, skip = datastart, header = TRUE)\n",
    "\n",
    "    first.year  <- as.numeric(format(as.Date(data$DATE[1]),'%Y'))\n",
    "    first.month <- as.numeric(format(as.Date(data$DATE[1]),'%m'))\n",
    "    \n",
    "    # Adjust frequency\n",
    "    if (freq.FRED == 'Quarterly'){\n",
    "        first.q  <- (first.month-1)/3 + 1\n",
    "        data.tis <- tis(data$VALUE, start = c(first.year, first.q), tif = 'quarterly')\n",
    "    } else if (freq.FRED == 'Monthly') {\n",
    "        data.tis <- tis(data$VALUE, start = c(first.year, first.month), tif = 'monthly')\n",
    "    }\n",
    "\n",
    "    # Convert frequency\n",
    "    if (freq.FRED == 'Monthly' & freq == 'Quarterly') {\n",
    "        data.tis <- convert(data.tis, tif = 'quarterly', method = 'constant', observed. = 'averaged')\n",
    "    }\n",
    "\n",
    "return(data.tis)} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function**: *splice*  \n",
    "**Description**: This function splices two series, with the series s2 beginning at splice.date and extended back using the growth rate at the splice.date times series s1. The freq argument accepts two values - 'quarterly' and 'monthly' - but it could be modified to take more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splice <- function(s1, s2, splice.date, freq) {\n",
    "    t <- splice.date #renaming for convenience\n",
    "    if (freq == \"quarterly\" | freq == \"Quarterly\") {\n",
    "        t.minus.1 <- shiftQuarter(t,-1)\n",
    "    }\n",
    "    else if (freq == \"monthly\" | freq == \"Monthly\") {\n",
    "        t.minus.1 <- shiftMonth(t,-1)\n",
    "    }\n",
    "    else { stop(\"You must enter 'quarterly' or 'monthly' for freq.\") }\n",
    "    ratio <- as.numeric(window(s2,start = t, end = t)/\n",
    "                        window(s1,start = t, end = t))\n",
    "\n",
    "return(mergeSeries(ratio*window(s1,end = t.minus.1),window(s2, start = t)))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function**: *gradient*  \n",
    "**Description**: This function computes the gradient of a function f given a vector input x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradient <- function(f, x, delta = x * 0 + 1.0e-5) {\n",
    "    g <- x * 0\n",
    "    for (i in 1:length(x)) {\n",
    "        x1 <- x\n",
    "        x1[i] <- x1[i] + delta[i]\n",
    "        f1 <- f(x1)\n",
    "        x2 <- x\n",
    "        x2[i] <- x2[i] - delta[i]\n",
    "        f2 <- f(x2)\n",
    "        g[i] <- (f1 - f2) / delta[i] / 2\n",
    "    }\n",
    "return(g)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages\n",
    "Checks for necessary R packages and installs them if not found. The \"tis\" package is used to manage time series data. The \"mFilter\" packages contains the hpfilter() function. We use the \"nloptr\" package for optimization. The \"seasonal\" package is an interface to the US Census Bureau software X-13-ARIMA-SEATS. It uses X-13 binaries which are installed by the dependent \"x13binary\" packages; users may need to manually specify the path to the binaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (!require(\"tis\")) {install.packages(\"tis\", lib = \"Rpackages\"); library(\"tis\", lib = \"Rpackages\")} # Time series package\n",
    "if (!require(\"mFilter\")) {install.packages(\"mFilter\", lib = \"Rpackages\"); library(\"mFilter\", lib = \"Rpackages\")} # HP filter\n",
    "if (!require(\"nloptr\")) {install.packages(\"nloptr\", lib = \"Rpackages\"); library(\"nloptr\", lib = \"Rpackages\")} # Optimization\n",
    "if (!require(\"seasonal\")) {install.packages(\"seasonal\", lib = \"Rpackages\"); library('seasonal', lib = \"Rpackages\")} # Seasonal adjustment\n",
    "Sys.setenv(X13_PATH = \"X13\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "We use four R scripts, one for each economy titled *prepare.rstar.data.XX.R*, to prepare the data to be used in the estimation. Inputs are the raw data described above. The files are described below. In the accompanying set of code, they are sourced in *run.hlw.R*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *prepare.rstar.data.us.R*\n",
    "This file compiles and prepares the data used in HLW for the US.\n",
    "\n",
    "Manually specify start and end dates for the prepared data to be used in the estimation. The variable *data.start* should be 4 quarters prior to the estimation start date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.start <- c(1960,1)\n",
    "data.end   <- c(2015,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data using the getFRED() function in *utilities.R*. For this connection to work, the user must have the *wget* command line utility. Alternatively, you can comment out the data import section of the file and manually download the data from FRED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gdp             <- getFRED(paste0('https://fred.stlouisfed.org/',\n",
    "                                     'data/GDPC1.txt'))\n",
    "\n",
    "price.index     <- getFRED(paste0('https://fred.stlouisfed.org/',                                  \n",
    "                                     'data/PCEPILFE.txt'))\n",
    "\n",
    "ny.discount     <- getFRED(paste0('https://fred.stlouisfed.org/',\n",
    "                                     'data/INTDSRUSM193N.txt'))\n",
    "\n",
    "fed.funds       <- getFRED(paste0('https://fred.stlouisfed.org/',\n",
    "                                     'data/FEDFUNDS.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data:\n",
    "* Take the log of real GDP.\n",
    "* Create an annualized core inflation series from the price index.\n",
    "* Construct a measure of inflation expectations as a four-quarter moving average of past inflation.\n",
    "* Express interest rate data on a 365-day basis.\n",
    "* Splice FRBNY discount rates with the federal funds rate in 1965:Q1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take log of real GDP\n",
    "gdp.log <- log(gdp)\n",
    "\n",
    "# Create an annualized inflation series using the price index\n",
    "inflation <- 400*log(price.index/Lag(price.index, k=1))\n",
    "\n",
    "# Inflation expectations measure: 4-quarter moving average of past inflation\n",
    "inflation.expectations <- (inflation + Lag(inflation, k=1) + Lag(inflation, k=2) + Lag(inflation, k=3))/4\n",
    "\n",
    "# Express interest rate data on a 365-day basis\n",
    "ny.discount.eff <- 100*((1+ny.discount/36000)^365 -1)\n",
    "fed.funds.eff   <- 100*((1+fed.funds/36000)^365 -1)\n",
    "\n",
    "# NY Fed discount rate is used prior to 1965; thereafter, use the effective federal funds rate\n",
    "interest <- mergeSeries(window(ny.discount.eff, end = c(1964,4)),window(fed.funds.eff, start = c(1965,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output data to *inputData/rstar.data.us.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.out <- window(cbind(gdp.log, inflation, inflation.expectations, interest),start = data.start, end = data.end)\n",
    "write.table(data.out,file = 'inputData/rstar.data.us.csv', sep = ',',\n",
    "            col.names = TRUE, quote = FALSE, na = '.', row.names = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *prepare.rstar.data.ca.R*  \n",
    "This file compiles and prepares the data used in HLW for Canada.\n",
    "\n",
    "Manually specify start and end dates for the prepared data to be used in the estimation. The variable *data.start* should be 4 quarters prior to the estimation start date. Manually specify the start dates for the two CPI series and the core CPI series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.start <- c(1960,1)\n",
    "data.end   <- c(2015,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data described in **Raw Data** section must be downloaded by the user prior to running this file. Code is set up such that each CANSIM series is in its own CSV with time as rows.\n",
    "\n",
    "**Import GDP Data**  \n",
    "**PRIOR STEPS**:  \n",
    "    1. Download data from the International Financial Statistics (IFS) Database on the IMF website:  \n",
    "       Series: \"Gross Domestic Product, Real, Seasonally adjusted, Index\"\n",
    "    2. Save data as a CSV and specify the file name in gdp.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gdp.file <- \"rawData/canada_gdp_ifs.csv\"\n",
    "gdp.data <- read.table(gdp.file, skip = 1, header = TRUE, sep = ',', stringsAsFactors = FALSE)\n",
    "gdp      <- tis(gdp.data$Canada, start = data.start, tif = 'quarterly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Bank Rate Data**  \n",
    "**PRIOR STEPS:**  \n",
    "    1. Download data from the Statistics Canada (CANSIM) website:  \n",
    "       Series: v122530 \n",
    "    2. Save data as a CSV and specify the file name in bank.rate.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bank.rate.file <- \"rawData/cansim_bank_rate_v122530.csv\"\n",
    "bank.rate.data <- read.table(bank.rate.file, skip = 2, header = TRUE, sep = ',', stringsAsFactors = FALSE)\n",
    "bank.rate.m    <- tis(bank.rate.data$v122530, start = data.start, tif='monthly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Target Rate Data**  \n",
    "**PRIOR STEPS:** \n",
    "    1. Download data from the Statistics Canada (CANSIM) website:  \n",
    "       Series: v39079        \n",
    "    2. Save data as a CSV and specify the file name in target.rate.file\n",
    "    3. Manually specify start date of downloaded target rate data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target.rate.start <- c(2001,1)\n",
    "target.rate.file  <- \"rawData/cansim_target_rate_v39079.csv\"\n",
    "target.rate.data  <- read.table(target.rate.file, skip = 2, header = TRUE, sep = ',', stringsAsFactors = FALSE)\n",
    "target.rate.data$Daily <- as.Date(target.rate.data$Daily, \"%b %d %Y\")\n",
    "\n",
    "# Format data as time series with daily frequency\n",
    "if (!require(\"xts\")) {install.packages(\"xts\"); library(\"xts\")}\n",
    "target.rate.d <- as.xts(target.rate.data$v39079, order.by = target.rate.data$Daily, frequency = 365)\n",
    "# Remove data with zero values (these indicate non-business days)\n",
    "target.rate.d <- target.rate.d[!(target.rate.d==0.00)]\n",
    "# Aggregate data to monthly frequency by taking end-of-period values\n",
    "target.rate.m <- apply.monthly(target.rate.d, tail, n=1)\n",
    "detach(\"package:xts\")\n",
    "detach(\"package:zoo\")\n",
    "# Format data as a tis time series with monthly frequency\n",
    "target.rate.m <- as.tis(data.frame(target.rate.m)$target.rate.m, start = target.rate.start, tif = 'monthly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Core CPI and CPI Data**  \n",
    "**PRIOR STEPS:** \n",
    "    1. Download data from the Statistics Canada (CANSIM) website:\n",
    "       Series: v41690926, v41690914, v41690973 (core CPI, CPI, and CPI to be used for backcasting, respectively)\n",
    "    2. Save data as a CSV and specify the file name in core.cpi.file, cpi.file, cpi.back.file\n",
    "    3. Manually specify start dates for each series.\n",
    "   \n",
    "**NOTES:**\n",
    "* Series v41690914 begins in Jan. 1992 and will be used as the CPI series thereafter;\n",
    "* Growth rates from series v41690973 are used to extend the CPI series back to 1959.\n",
    "* Series v41690914 and v41690926 are already seasonally adjusted but v41690973 is not, so we seasonally adjust it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "core.cpi.start <- c(1984,1)\n",
    "cpi.start      <- c(1992,1)\n",
    "cpi.back.start <- c(1959,1)\n",
    "\n",
    "core.cpi.file  <- \"rawData/cansim_core_cpi_v41690926.csv\"\n",
    "cpi.file       <- \"rawData/cansim_cpi_v41690914.csv\"\n",
    "cpi.back.file  <- \"rawData/cansim_cpi_back_v41690973.csv\"\n",
    "\n",
    "core.cpi.data  <- read.table(core.cpi.file, skip = 2, header = TRUE, sep = ',', stringsAsFactors = FALSE)\n",
    "cpi.data       <- read.table(cpi.file, skip = 2, header = TRUE, sep = ',', stringsAsFactors = FALSE)\n",
    "cpi.back.data  <- read.table(cpi.back.file, skip = 2, header = TRUE, sep = ',', stringsAsFactors = FALSE)\n",
    "\n",
    "# Format data as time series with monthly frequency\n",
    "core.cpi.m     <- tis(core.cpi.data$v41690926, start = core.cpi.start, tif = 'monthly')\n",
    "cpi.m          <- tis(cpi.data$v41690914, start = cpi.start, tif = 'monthly')\n",
    "cpi.back.m.nsa <- tis(cpi.back.data$v41690973, start = cpi.back.start, tif = 'monthly')\n",
    "\n",
    "# Seasonally adjust CPI Growth Rate data (other two series are already SA)\n",
    "cpi.back.m <- final(seas(as.ts(naWindow(cpi.back.m.nsa),freq=12)))\n",
    "cpi.back.m <- as.tis(cpi.back.m,start=cpi.back.start,tif='monthly') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data:\n",
    "* Take the log of real GDP.\n",
    "* Create an all-items price index using series v41690914 from Jan. 1992 to present. Uses growth rates of series v41690973 to extend the series back to Jan. 1959.\n",
    "* Create annualized inflation and core inflation series using the spliced CPI price index and core CPI price index, respectively.\n",
    "* Our inflation measure uses core inflation beginning in 1984Q2 and all-items inflation prior.\n",
    "* Construct a measure of inflation expectations as a four-quarter moving average of past inflation.\n",
    "* Create an interest rate series using the target rate beginning in May 2001 and the bank rate prior, and aggregates this series to quarterly frequency.\n",
    "* Express interest rates on a 365-day basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take log of real GDP\n",
    "gdp.log <- log(gdp)\n",
    "\n",
    "# CPI series: cpi.m is used from Jan 1992 to present and extended back to\n",
    "# Jan 1959 using growth rates with splice() function in utilities.R\n",
    "cpi.series.m <- splice(cpi.back.m, cpi.m, cpi.start, freq = 'monthly')\n",
    "\n",
    "# Aggregate core CPI and CPI series to quarterly frequency by taking the average\n",
    "core.cpi.q <- aggregate(core.cpi.m, nf = 4, FUN = mean)\n",
    "cpi.q      <- aggregate(cpi.series.m, nf = 4, FUN = mean)\n",
    "\n",
    "# Create annualized core inflation and inflation series using the price indices\n",
    "core.inflation.q <- 400*log(core.cpi.q/Lag(core.cpi.q, k=1))\n",
    "inflation.q      <- 400*log(cpi.q/Lag(cpi.q, k=1))\n",
    "\n",
    "# Final inflation series: CPI series is used prior to 1984q2;\n",
    "# thereafter, use core CPI series\n",
    "inflation <- mergeSeries(window(inflation.q, end = core.cpi.start),\n",
    "                         window(core.inflation.q, start = shiftQuarter(core.cpi.start,1)))\n",
    "\n",
    "# Inflation expectations measure: 4-quarter moving average of past inflation\n",
    "inflation.expectations <- (inflation + Lag(inflation, k=1) + Lag(inflation, k=2) + Lag(inflation, k=3))/4\n",
    "\n",
    "# Bank rate is used prior to May 2001; thereafter, use the target rate\n",
    "interest.m <- mergeSeries(window(bank.rate.m, end = c(2001,4)),window(target.rate.m, start = c(2001,5)))\n",
    "\n",
    "# Aggregate interest rate data to quarterly frequency by taking the average\n",
    "interest.q <- aggregate(interest.m, nf = 4, FUN = mean)\n",
    "\n",
    "# Express interest rate data on a 365-day basis\n",
    "interest <- 100*((1+interest.q/36000)^365 -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output data to *inputData/rstar.data.ca.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.out <- window(cbind(gdp.log, inflation, inflation.expectations, interest),start = data.start, end = data.end)\n",
    "write.table(data.out,file = 'inputData/rstar.data.ca.csv', sep = ',',\n",
    "            col.names = TRUE, quote = FALSE, na = '.', row.names = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *prepare.rstar.data.ea.R*\n",
    "This file compiles and prepares the data used in HLW for the Euro Area.\n",
    "\n",
    "Manually specify start and end dates for the prepared data to be used in the estimation. The variable *data.start* should be 4 quarters prior to the estimation start date. Note that in HLW the estimation sample begins later for the Euro Area than for other economies due to data availability. Manually specify the start date for the core CPI series (Mnemonic: HEX)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.start <- c(1971,1)\n",
    "data.end   <- c(2015,4)\n",
    "\n",
    "core.cpi.start <- c(1987,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Area Wide Model data described in **Raw Data** section must be downloaded by the user prior to running this file.\n",
    "\n",
    "**Import Area Wide Model Data**  \n",
    "**PRIOR STEPS:**  \n",
    "    1. Download data from the Euro Area Business Cycle Network website.\n",
    "    2. Save data as a CSV and specify the file name in awm.file.  \n",
    "\n",
    "**NOTE:** We seasonally adjust the non-seasonally-adjusted CPI and core CPI series (HICP and HEX) rather than using the provided seasonally adjusted series (HICPSA and HEXSA) because the latter series are only available for part of the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "awm.file <- 'rawData/area_wide_model_ea.csv'\n",
    "awm.data <- read.table(awm.file,header=TRUE,sep=',')[,c('DATE','YER','STN','HEX','HICP')]\n",
    "\n",
    "awm.start    <- c(as.numeric(substr(awm.data[1,'DATE'],1,4)),\n",
    "                  as.numeric(substr(awm.data[1,'DATE'],6,7)))\n",
    "\n",
    "gdp          <- tis(awm.data$YER, start = awm.start, tif = 'quarterly')\n",
    "\n",
    "data.end     <- end(as.ts(gdp))\n",
    "\n",
    "cpi.nsa      <- tis(awm.data$HICP, start = awm.start, tif = 'quarterly')\n",
    "core.cpi.nsa <- tis(awm.data$HEX, start = awm.start, tif = 'quarterly')\n",
    "core.cpi.nsa <- window(core.cpi.nsa, start = core.cpi.start)\n",
    "interest.q   <- tis(awm.data$STN, start = awm.start, tif = 'quarterly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import ECB Data (for extending series past AWM end date)**  \n",
    "**PRIOR STEPS:** \n",
    "    1. Download data from the ECB's Statistical Data Warehouse (see Sec. 3.3 for mnemonics).\n",
    "    2. Save data as CSV files and specify names in variable.file.\n",
    "    3. Ensure data is sorted with dates ascending.\n",
    "    4. Specify start dates and splice dates where applicable.\n",
    "    5. Splice GDP and interest series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gdp.file      <- 'rawData/ea_gdp.csv'\n",
    "gdp.data      <- read.table(gdp.file,header=FALSE,sep=',',skip=5)\n",
    "gdp.start.ecb <- c(1995,1)\n",
    "\n",
    "gdp.ecb       <- tis(gdp.data$V2, start = gdp.start.ecb, tif = 'quarterly')\n",
    "\n",
    "gdp.spliced   <- splice(gdp, gdp.ecb, gdp.start.ecb, \"Quarterly\")\n",
    "\n",
    "core.cpi.file <- 'rawData/ea_hicp.csv'\n",
    "core.cpi.data <- read.table(core.cpi.file,header=FALSE,sep=',',skip=5)\n",
    "core.cpi.start.ecb   <- c(1996,1)\n",
    "core.cpi.splice.date <- c(2016,1)\n",
    "\n",
    "core.cpi.nsa.m.ecb <- tis(core.cpi.data$V2, start = core.cpi.start.ecb, tif = 'monthly')\n",
    "\n",
    "interest.file <- 'rawData/ea_nominal_rate.csv'\n",
    "interest.data <- read.table(interest.file,header=FALSE,sep=',',skip=5)\n",
    "interest.start.ecb   <- c(1994,1)\n",
    "interest.splice.date <- c(2016,1)\n",
    "\n",
    "interest.q.ecb <- tis(interest.data$V2, start = interest.start.ecb, tif = 'quarterly')\n",
    "\n",
    "interest.spliced <- mergeSeries(window(interest.q, end = shiftQuarter(interest.splice.date,-1)),\n",
    "                                window(interest.q.ecb, start = interest.splice.date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data:\n",
    "* Take the log of real GDP.\n",
    "* Aggregate ECB core CPI data to quarterly from monthly frequency.\n",
    "* Splice ECB core CPI data with AWM core CPI data.\n",
    "* Create annualized inflation and core inflation series using the CPI price index and core CPI price index, respectively.\n",
    "* Our inflation measure uses core inflation beginning in 1988Q1 and all-items inflation prior.\n",
    "* Construct a measure of inflation expectations as a four-quarter moving average of past inflation.\n",
    "* Express interest rates on a 365-day basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take log of real GDP\n",
    "gdp.log <- log(gdp.spliced)\n",
    "\n",
    "# Aggregate ECB core CPI data to quarterly from monthly frequency\n",
    "core.cpi.nsa.ecb <- convert(core.cpi.nsa.m.ecb, tif = 'quarterly', observed = 'averaged')\n",
    "\n",
    "# Splice ECB core CPI data with AWM core CPI data in 2015q4\n",
    "core.cpi.nsa.spliced <- splice(core.cpi.nsa, core.cpi.nsa.ecb, shiftQuarter(core.cpi.splice.date, -1), \"Quarterly\")\n",
    "\n",
    "# Seasonally adjust CPI and core CPI data and re-format as tis series\n",
    "cpi <- final(seas(as.ts(naWindow(cpi.nsa),freq=4)))\n",
    "cpi <- as.tis(cpi,start=awm.start,tif='quarterly')\n",
    "\n",
    "core.cpi <- final(seas(as.ts(naWindow(core.cpi.nsa.spliced),freq=4)))\n",
    "core.cpi <- as.tis(core.cpi,start=core.cpi.start,tif='quarterly')\n",
    "\n",
    "# Create annualized core inflation and inflation series using the price indices\n",
    "core.inflation.q <- 400*log(core.cpi/Lag(core.cpi, k=1))\n",
    "inflation.q      <- 400*log(cpi/Lag(cpi, k=1))\n",
    "\n",
    "# Final inflation series: CPI series is used prior to 1988q1;\n",
    "# thereafter, use core CPI series\n",
    "inflation <- mergeSeries(window(inflation.q, end = core.cpi.start),window(core.inflation.q, start = shiftQuarter(core.cpi.start,1)))\n",
    "\n",
    "# Inflation expectations measure: 4-quarter moving average of past inflation\n",
    "inflation.expectations <- (inflation + Lag(inflation, k=1) + Lag(inflation, k=2) + Lag(inflation, k=3))/4\n",
    "\n",
    "# Express interest rate data on a 365-day basis\n",
    "interest <- 100*((1+interest.spliced/36000)^365 -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output data to *inputData/rstar.data.ea.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.out <- window(cbind(gdp.log, inflation, inflation.expectations, interest),start = data.start, end = data.end)\n",
    "write.table(data.out,file = 'inputData/rstar.data.ea.csv', sep = ',',\n",
    "            col.names = TRUE, quote = FALSE, na = '.', row.names = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *prepare.rstar.data.uk.R*\n",
    "This file compiles and prepares the data used in HLW for the UK.\n",
    "\n",
    "Manually specify start and end dates for the prepared data to be used in the estimation. The variable *data.start* should be 4 quarters prior to the estimation start date. Manually specify the start date for the core CPI series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.start <- c(1960,1)\n",
    "data.end   <- c(2015,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data described in **Raw Data** section must be downloaded by the user prior to running this file.\n",
    "\n",
    "** Import GDP Data**  \n",
    "**PRIOR STEPS:**  \n",
    "    1. Download data from the ONS website:\n",
    "       Series: ABMI: \"Gross Domestic Product: chained volume measures: Seasonally adjusted (Millions of pounds)\n",
    "    2. Save data as a CSV and specify the file name in gdp.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gdp.file  <- \"rawData/uk_gdp_ons_abmi.csv\"\n",
    "gdp.data  <- read.table(gdp.file, skip = 7, header = FALSE, sep = ',', stringsAsFactors = FALSE) \n",
    "gdp       <- tis(gdp.data$V2, start = data.start, tif='quarterly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Core CPI and CPI Data**  \n",
    "**PRIOR STEPS:** \n",
    "    1. Download data from the OECD website:\n",
    "       Source: Consumer Prices (MEI); Series: Consumer prices - all items; \n",
    "       Consumer prices - all items non-food, non-energy\n",
    "    2. Save both data series in one CSV and specify the file name in cpi.file.\n",
    "    3. Manually specify start dates for each series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "core.cpi.start <- c(1970,1)\n",
    "cpi.start      <- c(1959,1)\n",
    "\n",
    "cpi.file     <- \"rawData/uk_price_indices.csv\"\n",
    "cpi.data     <- read.table(cpi.file, skip = 0, header = TRUE, sep = ',', stringsAsFactors = FALSE)\n",
    "core.cpi.nsa <- tis(cpi.data$core_cpi, start = cpi.start, tif = 'quarterly')\n",
    "core.cpi.nsa <- window(core.cpi.nsa, start = core.cpi.start)\n",
    "cpi.nsa      <- tis(cpi.data$cpi, start = cpi.start, tif = 'quarterly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Bank Rate Data**  \n",
    "**PRIOR STEPS:**\n",
    "    1. Download data from the BOE website:\n",
    "       Bank of England Statistical Interactive Database - official Bank Rate history\n",
    "    2. Manually edit the historical since 1694 series \n",
    "       1. Include only 1960 to present, but enter 11/20/1958 change as 1/1/1960 so series has starting value\n",
    "       2. Populate rows with years. Remove sub-headers and rows of spaces\n",
    "          In Excel, select area and choose F5, \"Special\", \"Blanks\", Delete\n",
    "       3. Code assume the setup has 4 columns: \"year\",\"day\",\"month\",\"rate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bank.rate.file      <- \"rawData/uk_boe_bank_rate_changes.csv\"\n",
    "bank.rate.data      <- read.table(bank.rate.file, skip = 0, header = TRUE, sep = ',', stringsAsFactors = FALSE)\n",
    "bank.rate.data$date <- as.Date(paste(bank.rate.data$month,bank.rate.data$day,bank.rate.data$year), \"%b %d %Y\")\n",
    "bank.rate.data      <- subset(bank.rate.data, select = c(\"date\",\"rate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert series of changes in bank rate to a daily rate series:\n",
    "* Variable bank.rate.data contains changes in the BOE bank rate (dates are included only if a change occurs).\n",
    "* Variable bank.rate.changes.d is a daily time series with NA values inserted for dates on which a change does not occur.\n",
    "* Variable bank.rate.d is a daily series created by carrying forward the last value when a value is NA using the na.locf() function in the 'xts' package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date.seq            <- seq(from = bank.rate.data$date[1], to = as.Date(ti(data.end,tif='quarterly')), by = '1 day')\n",
    "if (!require(\"xts\")) {install.packages(\"xts\"); library(\"xts\")} # Time series library\n",
    "bank.rate.changes.d <- data.frame(bank.rate = with(bank.rate.data, rate[match(date.seq, date)]))\n",
    "bank.rate.d         <- as.xts(bank.rate.changes.d, order.by = date.seq, frequency = 365)\n",
    "bank.rate.d         <- na.locf(bank.rate.d)\n",
    "detach(\"package:xts\") # Remove packages to avoid masking base functions\n",
    "detach(\"package:zoo\")\n",
    "bank.rate.d         <- tis(bank.rate.d$bank.rate, end = ti(data.end, tif='quarterly'), tif = 'daily')[,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data:\n",
    "* Take the log of real GDP.\n",
    "* Seasonally adjust the CPI and core CPI series.\n",
    "* Create annualized inflation and core inflation series using the CPI price index and core CPI price index, respectively.\n",
    "* Our inflation measure uses core inflation beginning in 1970Q2 and all-items inflation prior.\n",
    "* Construct a measure of inflation expectations as a four-quarter moving average of past inflation.\n",
    "* Aggregate the bank rate series to quarterly frequency and express interest rates on a 365-day basis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take log of real GDP\n",
    "gdp.log <- log(gdp)\n",
    "\n",
    "# Seasonally adjust CPI and core CPI data and re-format as tis series\n",
    "cpi <- final(seas(as.ts(naWindow(cpi.nsa),freq=4)))\n",
    "cpi <- as.tis(cpi,start=cpi.start,tif='quarterly')\n",
    "\n",
    "core.cpi <- final(seas(as.ts(naWindow(core.cpi.nsa),freq=4)))\n",
    "core.cpi <- as.tis(core.cpi,start=core.cpi.start,tif='quarterly')\n",
    "\n",
    "# Create annualized core inflation and inflation series using the price indices\n",
    "core.inflation.q <- 400*log(core.cpi/Lag(core.cpi, k=1))\n",
    "inflation.q      <- 400*log(cpi/Lag(cpi, k=1))\n",
    "\n",
    "# Final inflation series: CPI series is used prior to 1970q2;\n",
    "# thereafter, use core CPI series\n",
    "inflation <- mergeSeries(window(inflation.q, end = core.cpi.start),\n",
    "                         window(core.inflation.q, start = shiftQuarter(core.cpi.start,1)))\n",
    "\n",
    "# Inflation expectations measure: 4-quarter moving average of past inflation\n",
    "inflation.expectations <- (inflation + Lag(inflation, k=1) + Lag(inflation, k=2) + Lag(inflation, k=3))/4\n",
    "\n",
    "# Aggregate bank rate data to quarterly frequency by taking the average\n",
    "interest.q <- convert(bank.rate.d, tif = 'quarterly', observed = 'averaged')\n",
    "\n",
    "# Express interest rate data on a 365-day basis\n",
    "interest <- 100*((1+interest.q/36000)^365 -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output data to *inputData/rstar.data.uk.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.out <- window(cbind(gdp.log, inflation, inflation.expectations, interest),start = data.start, end = data.end)\n",
    "write.table(data.out,file = 'inputData/rstar.data.uk.csv', sep = ',',\n",
    "            col.names = TRUE, quote = FALSE, na = '.', row.names = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Variables \n",
    "In this section, the sample period, constraints, and variables to be used throughout the estimation are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Upper bound on a_3 parameter (slope of the IS curve)\n",
    "a3.constraint <- -0.0025\n",
    "\n",
    "# Lower bound on b_2 parameter (slope of the Phillips curve)\n",
    "b2.constraint <- 0.025\n",
    "\n",
    "# Set the start and end dates of the estimation sample (format is c(year,quarter))\n",
    "sample.start <- c(1961,1)\n",
    "sample.end   <- c(2015,4)\n",
    "\n",
    "# Set the estimation sample start date for the Euro Area \n",
    "ea.sample.start <- c(1972,1)\n",
    "\n",
    "# The estimation process uses data beginning 4 quarters prior to the sample start\n",
    "data.start    <- shiftQuarter(sample.start,-4)\n",
    "ea.data.start <- shiftQuarter(ea.sample.start,-4)\n",
    "\n",
    "# Set start index for y\n",
    "g.pot.start.index <- 1 + ti(shiftQuarter(sample.start,-3),'quarterly')-ti(data.start,'quarterly')\n",
    "\n",
    "# Set column names for CSV output\n",
    "output.col.names <- c(\"Date\",\"rstar\",\"g\",\"z\",\"output gap\",\"\",\n",
    "                      \"All results are output from the Stage 3 model.\",rep(\"\",8),\n",
    "                      \"Standard Errors\",\"Date\",\"y*\",\"r*\",\"g\",\"\",\"rrgap\")\n",
    "\n",
    "# Set number of iterations for Monte Carlo standard error procedure\n",
    "niter <- 5000\n",
    "\n",
    "# Because the MC standard error procedure is time consuming, we include a run switch\n",
    "# Set run.se to TRUE to run the procedure\n",
    "run.se <- TRUE\n",
    "\n",
    "# Removes the seed setting if one exists\n",
    "rm(.Random.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation\n",
    "The results reported in Holston, Laubach, and Williams (2017) are based on our estimation method described in Section 2.2 of the paper. The estimation proceeds in sequential steps through three stages, each of which is implemented in an R program. These programs, as well as programs to obtain median unbiased estimates of the signal-to-noise ratios, apply the Kalman filter, and estimate parameters using maximum likelihood are described and included below.\n",
    "\n",
    "### Main Estimation Program: *run.hlw.estimation.R*\n",
    "The function *run.hlw.estimation.R* is called by *run.hlw.R* once for each economy. It takes as inputs the key variables for the given economy: log output, inflation, and the real and nominal short-term interest rates, as well as the specified constraints on $a_r$ and $b_y$. It calls the programs *rstar.stage1.R*, *rstar.stage2.R*, and *rstar.stage3.R* to run the three stages of the HLW estimation and returns output for each stage. Additionally, it calls the programs *median.unbiased.estimator.stage1.R* and *median.unbiased.estimator.stage2.R* to obtain the signal-to-noise ratios $\\lambda_g$ and $\\lambda_z$ and returns their values.\n",
    "\n",
    "The programs *unpack.parameters.stage1.R*, *unpack.parameters.stage2.R*, and *unpack.parameters.stage3.R* set up coefficient matrices for the corresponding state-space models for the given parameter vectors.\n",
    "\n",
    "In all stages, we impose the constraint $b_y \\geq 0.025$. In stages 2 and 3, we impose $a_r \\leq -0.0025$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run.hlw.estimation <- function(log.output, inflation, real.interest.rate, nominal.interest.rate,\n",
    "                               a3.constraint = NA, b2.constraint = NA, run.se = TRUE) {\n",
    "    # Running the stage 1 model\n",
    "    out.stage1 <- rstar.stage1(log.output,\n",
    "                               inflation,\n",
    "                               b2.constraint)\n",
    "\n",
    "    # Median unbiased estimate of lambda_g\n",
    "    lambda.g <- median.unbiased.estimator.stage1(out.stage1$potential.smoothed)\n",
    "\n",
    "    # Running the stage 2 model\n",
    "    out.stage2 <- rstar.stage2(log.output,\n",
    "                               inflation,\n",
    "                               real.interest.rate,\n",
    "                               lambda.g,\n",
    "                               a3.constraint,\n",
    "                               b2.constraint)\n",
    "    \n",
    "    # Median unbiased estimate of lambda_z\n",
    "    lambda.z <- median.unbiased.estimator.stage2(out.stage2$y, out.stage2$x)\n",
    "\n",
    "    # Running the stage 3 model\n",
    "    out.stage3 <- rstar.stage3(log.output,\n",
    "                               inflation,\n",
    "                               real.interest.rate,\n",
    "                               nominal.interest.rate,\n",
    "                               lambda.g,\n",
    "                               lambda.z,\n",
    "                               a3.constraint,\n",
    "                               b2.constraint,\n",
    "                               run.se)\n",
    "\n",
    "    return(list(out.stage1=out.stage1,out.stage2=out.stage2,out.stage3=out.stage3,\n",
    "                lambda.g=lambda.g,lambda.z=lambda.z))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Stage 1, 2, and 3 State-Space Models\n",
    "This sections presents the state-space models, and the next section documents the corresponding R programs. Notation matches that of Hamilton (1994) and is also used in the R programs. All of the state-space models can be cast in the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{y}_t = \\mathbf{A}' \\cdot \\mathbf{x}_t + \\mathbf{H}' \\cdot \\xi_t + \\mathbf{v}_t\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\xi_t = \\mathbf{F} \\cdot \\xi_{t-1} + \\epsilon_t\n",
    "\\end{equation}\n",
    "\n",
    "Here, $\\mathbf{y}_t$ is a vector of contemporaneous endogenous variables, while $\\mathbf{x}_t$ is a vector of exogenous and lagged exogenous variables. $\\xi_t$ is vector of unobserved states. The vectors of stochastic disturbances $\\mathbf{v}_t$ and $\\epsilon_t$ are assumed to be Gaussian and mututally uncorrelated, with mean zero and covariance matrices $\\mathbf{R}$ and $\\mathbf{Q}$, respectively. The covariance matrix $\\mathbf{R}$ is always assumed to be diagonal.\n",
    "\n",
    "For each model, there is a corresponding vector of parameters to be estimated by maximum likelihood. Because maximum likelihood estimates of the innovations to $g$ and $z$, $\\sigma_g$ and $\\sigma_z$, are likely to be biased towards zero (see Section 2.2 of HLW for explanation), we use Stock and Watson's (1998) mediun unbiased estimator to obtain estimates of two ratios, $\\lambda_g \\equiv \\frac{\\sigma_g}{\\sigma_{y^*}}$ and $\\lambda_z \\equiv \\frac{a_r \\sigma_z}{\\sigma_{\\tilde{y}}}$. We impose these ratios when estimating the remaining model parameters by maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Stage 1 Model\n",
    "The first-stage model, which corresponds to the *rstar.stage1.R* program, can be represented by the following matrices: \n",
    "\n",
    "$$ \\mathbf{y}_t = \\left[y_t, \\pi_t\\right]' $$\n",
    "\n",
    "$$ \\mathbf{x}_t = \\left[y_{t-1},y_{t-2},\\pi_{t-1},\\pi_{t-2,4}\\right]' $$\n",
    "\n",
    "$$ \\mathbf{\\xi}_t = \\left[y_t^*,y_{t-1}^*,y_{t-2}^*\\right]' $$\n",
    "\n",
    "\n",
    "$$ \\mathbf{H}' = \\begin{bmatrix} 1 & -a_{y,1} & -a_{y,2} \\\\ 0 & -b_y & 0 \\end{bmatrix} $$\n",
    "\n",
    "$$ \\mathbf{A}' = \\begin{bmatrix} a_{y,1} & a_{y,2} & 0 & 0 \\\\ b_y & 0 & b_\\pi & 1-b_\\pi \\end{bmatrix} $$\n",
    "\n",
    "$$ \\mathbf{F} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix} $$\n",
    "\n",
    "$$ \\mathbf{Q} = \\begin{bmatrix} \\sigma_{y*}^2 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix} $$\n",
    "\n",
    "\n",
    "The vector of parameters to be estimated by maximum likelihood is as follows:\n",
    "\n",
    "$$\\theta_1=\\left[a_{y,1},a_{y,2},b_\\pi,b_y,g,\\sigma_{\\widetilde{y}},\\sigma_\\pi,\\sigma_{y*}\\right]$$  \n",
    "\n",
    "### *unpack.parameters.stage1.R*\n",
    "This function generates the above matrices given $\\theta_1$ as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unpack.parameters.stage1 <- function(parameters, y.data, x.data, xi.00, P.00) {\n",
    "  A         <- matrix(0, 4, 2)\n",
    "  A[1:2, 1] <- parameters[1:2]  # a_y,1, a_y,2\n",
    "  A[1, 2]   <- parameters[4]    # b_y\n",
    "  A[3, 2]   <- parameters[3]    # b_pi\n",
    "  A[4, 2]   <- 1-parameters[3]  # 1 - b_pi\n",
    "  \n",
    "  H         <- matrix(0, 3, 2)  \n",
    "  H[1, 1]   <- 1\n",
    "  H[2:3, 1] <- -parameters[1:2] # -a_y,1, -a_y,2\n",
    "  H[2, 2]   <- -parameters[4]   # -b_y\n",
    "\n",
    "  R         <- diag(c(parameters[6]^2, parameters[7]^2)) # sigma_y~, sigma_pi\n",
    "  Q         <- matrix(0, 3, 3)\n",
    "  Q[1, 1]   <- parameters[8]^2  # sigma_y*\n",
    "\n",
    "  F <- matrix(0, 3, 3)\n",
    "  F[1, 1] <- F[2, 1] <- F[3, 2] <- 1\n",
    "\n",
    "  # Make the data stationary\n",
    "  y.data[, 1] <- y.data[, 1] - 1:dim(y.data)[1] * parameters[5]\n",
    "  x.data[, 1] <- x.data[, 1] - 0:(dim(x.data)[1]-1) * parameters[5]\n",
    "  x.data[, 2] <- x.data[, 2] - -1:(dim(x.data)[1]-2) * parameters[5]\n",
    "      \n",
    "return(list(\"xi.00\"=xi.00, \"P.00\"=P.00, \"F\"=F, \"Q\"=Q, \"A\"=A, \"H\"=H, \"R\"=R, \"x.data\"=x.data, \"y.data\"=y.data))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Stage 2 Model\n",
    "The second-stage model, which corresponds to the *rstar.stage2.R* program, can be represented by the following matrices:\n",
    "\n",
    "$$ \\mathbf{y}_t = \\left[y_{t},\\pi_{t}\\right]' $$\n",
    "\n",
    "$$ \\mathbf{x}_t = \\left[y_{t-1},y_{t-2},r_{t-1},r_{t-2},\\pi_{t-1},\\pi_{t-2,4},1\\right]' $$ \n",
    "\n",
    "$$ \\mathbf{\\xi}_t = \\left[y_t^*,y_{t-1}^*,y_{t-2}^*,g_{t-1}\\right]' $$\n",
    "\n",
    "$$ \\mathbf{H}' = \\begin{bmatrix} 1 & -a_{y,1} & -a_{y,2} & a_g \\\\ 0 & -b_y & 0 & 0 \\end{bmatrix} $$\n",
    "\n",
    "$$ \\mathbf{A}' = \\begin{bmatrix} a_{y,1} & a_{y,2} & \\frac{a_r}{2} & \\frac{a_r}{2} & 0 & 0 & a_0 \\\\ b_y & 0 & 0 & 0 & b_\\pi & 1-b_\\pi & 0 \\end{bmatrix} $$\n",
    "\n",
    "$$ \\mathbf{F} = \\begin{bmatrix} 1 & 0 & 0 & 1 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} $$ \n",
    "\n",
    "$$ \\mathbf{Q} = \\begin{bmatrix} \\sigma_{y*}^2 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & \\left(\\lambda_g\\sigma_{y*}\\right)^2 \\end{bmatrix} $$\n",
    "\n",
    "\n",
    "The vector of parameters to be estimated by maximum likelihood is as follows: \n",
    "\n",
    "$$ \\theta_2=\\left[a_{y,1},a_{y,2},a_r,a_0,a_g,b_\\pi,b_y,\\sigma_{\\widetilde{y}},\\sigma_\\pi,\\sigma_{y*}\\right] $$  \n",
    "\n",
    "### *unpack.parameters.stage2.R*\n",
    "This function generates the above matrices given $\\theta_2$ as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unpack.parameters.stage2 <- function(parameters, y.data, x.data, lambda.g, xi.00=NA, P.00=NA) {\n",
    "  A         <- matrix(0, 2, 7)\n",
    "  A[1, 1:2] <- parameters[1:2]   # a_y,1, a_y,2\n",
    "  A[1, 3:4] <- parameters[3]/2   # a_r/2\n",
    "  A[1, 7]   <- parameters[4]     # a_0\n",
    "  A[2, 1]   <- parameters[7]     # b_y\n",
    "  A[2, 5]   <- parameters[6]     # b_pi\n",
    "  A[2, 6]   <- 1 - parameters[6] # 1 - b_pi\n",
    "  A         <- t(A)\n",
    "  \n",
    "  H         <- matrix(0, 2, 4)\n",
    "  H[1, 1  ] <- 1\n",
    "  H[1, 2:3] <- -parameters[1:2] # -a_y,1, -a_y,2\n",
    "  H[1, 4  ] <- parameters[5]    # a_g\n",
    "  H[2, 2]   <- -parameters[7]   # -b_y\n",
    "  H         <- t(H)\n",
    "\n",
    "  R         <- diag(c(parameters[8]^2, parameters[9]^2)) # sigma_y~, sigma_pi\n",
    "  Q         <- matrix(0, 4, 4)\n",
    "  Q[1, 1]   <- parameters[10]^2              # sigma_y*\n",
    "  Q[4, 4]   <- (lambda.g * parameters[10])^2 # sigma_y* \n",
    "\n",
    "  F <- matrix(0, 4, 4)\n",
    "  F[1, 1] <- F[1, 4] <- F[2, 1] <- F[3, 2] <- F[4,4] <- 1\n",
    "  \n",
    "return(list(\"xi.00\"=xi.00, \"P.00\"=P.00, \"F\"=F, \"Q\"=Q, \"A\"=A, \"H\"=H, \"R\"=R, \"x.data\"=x.data, \"y.data\"=y.data))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Stage 3 Model\n",
    "The third-stage model, which corresponds to the *rstar.stage3.R* program, can be represented by the following matrices:\n",
    "\n",
    "$$ \\mathbf{y}_t = \\left[y_{t},\\pi_{t}\\right]' $$\n",
    "\n",
    "$$ \\mathbf{x}_t = \\left[y_{t-1},y_{t-2},r_{t-1},r_{t-2},\\pi_{t-1},\\pi_{t-2,4}\\right]' $$\n",
    "\n",
    "$$ \\mathbf{\\xi}_t = \\left[y_t^*,y_{t-1}^*,y_{t-2}^*,g_{t-1},g_{t-2},z_{t-1},z_{t-2}\\right]' $$\n",
    "\n",
    "$$ \\mathbf{H}' = \\begin{bmatrix} 1 & -a_{y,1} & -a_{y,2} & \\frac{-a_r}{2} & \\frac{-a_r}{2} & \\frac{-a_r}{2} & \\frac{-a_r}{2} \\\\ 0 & -b_y & 0 & 0 & 0 & 0 & 0 \\end{bmatrix} $$\n",
    "\n",
    "$$ \\mathbf{A}' = \\begin{bmatrix} a_1 & a_2 & \\frac{a_r}{2} & \\frac{a_r}{2} & 0 & 0 \\\\ b_y & 0 & 0 & 0 & b_\\pi & 1-b_\\pi \\end{bmatrix} $$\n",
    "\n",
    "$$ \\mathbf{F} = \\begin{bmatrix} 1 & 0 & 0 & 1 & 0 & 0 & 0 \\\\ 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 1 & 0 \\end{bmatrix} $$\n",
    "\n",
    "$$ \\mathbf{Q} = \\begin{bmatrix} \\left(1+\\lambda_g^2\\right)\\sigma_{y*}^2 & 0 & 0 & \\left(\\lambda_g\\sigma_{y*}\\right)^2 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ \\left(\\lambda_g\\sigma_{y*}\\right)^2 & 0 & 0 & \\left(\\lambda_g\\sigma_{y*}\\right)^2 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & \\left(\\frac{\\lambda_z\\sigma_{\\widetilde{y}}}{a_r}\\right)^2 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 \\end{bmatrix} $$\n",
    "\n",
    "The vector of parameters to be estimated by maximum likelihood is as follows:\n",
    "$$\\theta_3=\\left[a_{y,1},a_{y,2},a_r,b_\\pi,b_y,\\sigma_{\\widetilde{y}},\\sigma_\\pi,\\sigma_{y*}\\right]$$  \n",
    "\n",
    "### *unpack.parameters.stage3.R*\n",
    "This function generates the coefficient matrices above given $\\theta_3$ as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unpack.parameters.stage3 <- function(parameters, y.data, x.data, lambda.g, lambda.z, xi.00, P.00) {\n",
    "  A         <- matrix(0, 2, 6)   \n",
    "  A[1, 1:2] <- parameters[1:2]   # a_y,1, a_y,2\n",
    "  A[1, 3:4] <- parameters[3]/2   # a_r/2\n",
    "  A[2, 1]   <- parameters[5]     # b_y\n",
    "  A[2, 5]   <- parameters[4]     # b_pi\n",
    "  A[2, 6]   <- 1 - parameters[4] # 1 - b_pi\n",
    "  A         <- t(A)\n",
    "\n",
    "  \n",
    "  H         <- matrix(0, 2, 7)\n",
    "  H[1, 1]   <- 1\n",
    "  H[1, 2:3] <- -parameters[1:2]       # a_y,1, a_y,2\n",
    "  H[1, 4:5] <- -parameters[3] * 2     # -a_r/2 (annualized)\n",
    "  H[1, 6:7] <- -parameters[3]/2       # -a_r/2\n",
    "  H[2, 2]   <- -parameters[5]         # -b_y\n",
    "  H         <- t(H)\n",
    "\n",
    "  R         <- diag(c(parameters[6]^2, parameters[7]^2)) # sigma_y~, sigma_pi\n",
    "\n",
    "  Q         <- matrix(0, 7, 7)\n",
    "  Q[1, 1]   <- (1+lambda.g^2)*parameters[8]^2                  # sigma_y*\n",
    "  Q[1, 4]   <- Q[4, 1] <- Q[4, 4]<- (lambda.g*parameters[8])^2 # sigma_y*\n",
    "  Q[6, 6]   <- (lambda.z*parameters[6]/parameters[3])^2        # sigma_y~/a_r\n",
    "\n",
    "  F <- matrix(0, 7, 7)\n",
    "  F[1, 1] <- F[1, 4] <- F[2, 1] <- F[3, 2] <- F[4,4] <- F[5,4]<- F[6,6] <- F[7,6] <- 1\n",
    " \n",
    "return(list(\"xi.00\"=xi.00, \"P.00\"=P.00, \"F\"=F, \"Q\"=Q, \"A\"=A, \"H\"=H, \"R\"=R, \"x.data\"=x.data, \"y.data\"=y.data))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R Programs to Run the State-Space Models\n",
    "\n",
    "### *rstar.stage1.R*\n",
    "This function runs the model in the first stage of the HLW estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rstar.stage1 <- function(log.output,\n",
    "                         inflation,\n",
    "                         b2.constraint=NA) {\n",
    "\n",
    "  stage <- 1\n",
    "  \n",
    "  # Data must start 4 quarters before the estimation period\n",
    "  T <- length(log.output) - 4\n",
    "\n",
    "  # Original output gap estimate\n",
    "  x.og <- cbind(rep(1,T+4), 1:(T+4))\n",
    "  y.og <- log.output\n",
    "  output.gap <- (y.og - x.og %*% solve(t(x.og) %*% x.og, t(x.og) %*% y.og)) * 100\n",
    "\n",
    "  # Initialization of state vector for Kalman filter using HP trend of log output\n",
    "  log.output.hp.trend <- hpfilter(log.output,freq=36000,type=\"lambda\",drift=FALSE)$trend\n",
    "  g.pot <- log.output.hp.trend[(g.pot.start.index):length(log.output.hp.trend)]\n",
    "  xi.00 <- c(100*g.pot[3:1])\n",
    "\n",
    "  # IS curve\n",
    "  y.is <- output.gap[5:(T+4)]\n",
    "  x.is <- cbind(output.gap[4:(T+3)], output.gap[3:(T+2)])\n",
    "  b.is <- solve(t(x.is) %*% x.is, t(x.is) %*% y.is)\n",
    "  r.is <- y.is - x.is %*% b.is\n",
    "  s.is <- sqrt(sum(r.is^2) / (length(r.is)-(dim(x.is)[2])))\n",
    "\n",
    "  # Phillips curve\n",
    "  y.ph <- inflation[5:(T+4)]\n",
    "  x.ph <- cbind(inflation[4:(T+3)],\n",
    "                (inflation[3:(T+2)]+inflation[2:(T+1)]+inflation[1:T])/3,\n",
    "                output.gap[4:(T+3)])                \n",
    "  b.ph <- solve(t(x.ph) %*% x.ph, t(x.ph) %*% y.ph)\n",
    "  r.ph <- y.ph - x.ph %*% b.ph\n",
    "  s.ph <- sqrt(sum(r.ph^2) / (length(r.ph)-(dim(x.ph)[2])))\n",
    "  \n",
    "  y.data <- cbind(100 * log.output[5:(T+4)],\n",
    "                  inflation[5:(T+4)])\n",
    "  x.data <- cbind(100 * log.output[4:(T+3)],\n",
    "                  100 * log.output[3:(T+2)],\n",
    "                  inflation[4:(T+3)],\n",
    "                  (inflation[3:(T+2)]+inflation[2:(T+1)]+inflation[1:T])/3)\n",
    "\n",
    "  # Starting values for the parameter vector\n",
    "  initial.parameters <- c(b.is, b.ph[1], b.ph[3], 0.85, s.is, s.ph, 0.5)\n",
    "\n",
    "  # Set an upper and lower bound on the parameter vectors:\n",
    "  # The vector is unbounded unless values are otherwise specified\n",
    "  theta.lb <- c(rep(-Inf,length(initial.parameters)))\n",
    "  theta.ub <- c(rep(Inf,length(initial.parameters)))\n",
    "  \n",
    "  # Set a lower bound for the Phillips curve slope (b_2) of b2.constraint, if not NA\n",
    "  # In HLW, b2.constraint = 0.025\n",
    "  if (!is.na(b2.constraint)) {\n",
    "      if (initial.parameters[4] < b2.constraint) {\n",
    "          initial.parameters[4] <- b2.constraint\n",
    "      }\n",
    "      theta.lb[4] <- b2.constraint\n",
    "  }\n",
    "\n",
    "  # Set the initial covariance matrix (see footnote 6) \n",
    "  P.00 <- calculate.covariance(initial.parameters, theta.lb, theta.ub, y.data, x.data, stage, NA, NA, xi.00)\n",
    "  \n",
    "  # Get parameter estimates via maximum likelihood\n",
    "  f <- function(theta) {return(-log.likelihood.wrapper(theta, y.data, x.data, stage, NA, NA, xi.00, P.00)$ll.cum)}\n",
    "  nloptr.out <- nloptr(initial.parameters, f, eval_grad_f=function(x) {gradient(f, x)},\n",
    "                       lb=theta.lb,ub=theta.ub,\n",
    "                       opts=list(\"algorithm\"=\"NLOPT_LD_LBFGS\",\"xtol_rel\"=1.0e-8))\n",
    "  theta <- nloptr.out$solution\n",
    "\n",
    "  log.likelihood <- log.likelihood.wrapper(theta, y.data, x.data, stage, NA, NA, xi.00, P.00)$ll.cum\n",
    "\n",
    "  # Get state vectors (xi.tt, xi.ttm1, xi.tT, P.tt, P.ttm1, P.tT) via Kalman filter\n",
    "  states <- kalman.states.wrapper(theta, y.data, x.data, stage, NA, NA, xi.00, P.00)\n",
    "\n",
    "  # One-sided (filtered) estimates  \n",
    "  potential.filtered  <- states$filtered$xi.tt[,1]/100\n",
    "  output.gap.filtered <- y.data[,1] - (potential.filtered * 100)\n",
    "  \n",
    "  # Two-sided (smoothed) estimates\n",
    "  potential.smoothed  <- as.vector(states$smoothed$xi.tT[,1])/100\n",
    "  output.gap.smoothed <- y.data[,1] - (potential.smoothed * 100)\n",
    "\n",
    "  # Save variables to return\n",
    "  return.list                <- list()\n",
    "  return.list$theta          <- theta\n",
    "  return.list$log.likelihood <- log.likelihood\n",
    "  return.list$states         <- states\n",
    "  return.list$xi.00          <- xi.00\n",
    "  return.list$P.00           <- P.00\n",
    "  return.list$potential.filtered  <- potential.filtered\n",
    "  return.list$output.gap.filtered <- output.gap.filtered\n",
    "  return.list$potential.smoothed  <- potential.smoothed\n",
    "  return.list$output.gap.smoothed <- output.gap.smoothed\n",
    "  return(return.list)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *rstar.stage2.R*\n",
    "This function runs the model in the second stage of the HLW estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rstar.stage2 <- function(log.output,\n",
    "                         inflation,\n",
    "                         real.interest.rate,\n",
    "                         lambda.g,\n",
    "                         a3.constraint=NA,\n",
    "                         b2.constraint=NA) {\n",
    "                         \n",
    "  stage <- 2\n",
    "\n",
    "  # Data must start 4 quarters before the estimation period  \n",
    "  T <- length(log.output) - 4\n",
    "\n",
    "  # Original output gap estimate\n",
    "  x.og <- cbind(rep(1,T+4), 1:(T+4))\n",
    "  y.og <- log.output\n",
    "  output.gap <- (y.og - x.og %*% solve(t(x.og) %*% x.og, t(x.og) %*% y.og)) * 100\n",
    "\n",
    "  # Initialization of state vector for Kalman filter using HP trend of log output\n",
    "  log.output.hp.trend <- hpfilter(log.output,freq=36000,type=\"lambda\",drift=FALSE)$trend\n",
    "  g.pot <- log.output.hp.trend[(g.pot.start.index):length(log.output.hp.trend)]\n",
    "  g.pot.diff <- diff(g.pot)\n",
    "  xi.00 <- c(100*g.pot[3:1],100*g.pot.diff[2])\n",
    "\n",
    "  # IS curve\n",
    "  y.is <- output.gap[5:(T+4)]\n",
    "  x.is <- cbind(output.gap[4:(T+3)], output.gap[3:(T+2)],\n",
    "                (real.interest.rate[4:(T+3)] + real.interest.rate[3:(T+2)])/2,\n",
    "                rep(1,T))\n",
    "  b.is <- solve(t(x.is) %*% x.is, t(x.is) %*% y.is)\n",
    "  r.is <- as.vector(y.is - x.is %*% b.is)\n",
    "  s.is <- sqrt(sum(r.is^2) / (length(r.is)-(dim(x.is)[2])))\n",
    "\n",
    "  # Phillips curve\n",
    "  y.ph <- inflation[5:(T+4)]\n",
    "  x.ph <- cbind(inflation[4:(T+3)],\n",
    "                (inflation[3:(T+2)]+inflation[2:(T+1)]+inflation[1:T])/3,\n",
    "                output.gap[4:(T+3)])\n",
    "  b.ph <- solve(t(x.ph) %*% x.ph, t(x.ph) %*% y.ph)\n",
    "  r.ph <- y.ph - x.ph %*% b.ph\n",
    "  s.ph <- sqrt(sum(r.ph^2) / (length(r.ph)-(dim(x.ph)[2])))\n",
    "  \n",
    "  y.data <- cbind(100 * log.output[5:(T+4)],\n",
    "                  inflation[5:(T+4)])\n",
    "  x.data <- cbind(100 * log.output[4:(T+3)],\n",
    "                  100 * log.output[3:(T+2)],\n",
    "                  real.interest.rate[4:(T+3)],\n",
    "                  real.interest.rate[3:(T+2)],                  \n",
    "                  inflation[4:(T+3)],\n",
    "                  (inflation[3:(T+2)]+inflation[2:(T+1)]+inflation[1:T])/3,\n",
    "                  rep(1,T))\n",
    "\n",
    "  # Starting values for the parameter vector\n",
    "  initial.parameters <- c(b.is, -b.is[3], b.ph[1], b.ph[3], s.is, s.ph, 0.5)\n",
    "\n",
    "  # Set an upper and lower bound on the parameter vectors:\n",
    "  # The vector is unbounded unless values are otherwise specified\n",
    "  theta.lb <- c(rep(-Inf,length(initial.parameters)))\n",
    "  theta.ub <- c(rep(Inf,length(initial.parameters)))\n",
    "\n",
    "  # Set a lower bound for the Phillips curve slope (b_2) of b2.constraint, if not NA\n",
    "  # In HLW, b2.constraint = 0.025\n",
    "  if (!is.na(b2.constraint)) {\n",
    "      if (initial.parameters[7] < b2.constraint) {\n",
    "          initial.parameters[7] <- b2.constraint\n",
    "      }\n",
    "      theta.lb[7] <- b2.constraint\n",
    "  }\n",
    "\n",
    "  # Set an upper bound for the IS curve slope (a_3) of a3.constraint, if not NA\n",
    "  # In HLW, a3.constraint = -0.0025\n",
    "  if (!is.na(a3.constraint)) {\n",
    "      if (initial.parameters[3] > a3.constraint) {\n",
    "          initial.parameters[3] <- a3.constraint\n",
    "      }\n",
    "      theta.ub[3] <- a3.constraint      \n",
    "  }\n",
    "\n",
    "  # Set the initial covariance matrix (see footnote 6)\n",
    "  P.00 <- calculate.covariance(initial.parameters, theta.lb, theta.ub, y.data, x.data, stage, lambda.g, NA, xi.00)\n",
    "  \n",
    "  # Get parameter estimates via maximum likelihood\n",
    "  f <- function(theta) {return(-log.likelihood.wrapper(theta, y.data, x.data, stage, lambda.g, NA, xi.00, P.00)$ll.cum)}\n",
    "  nloptr.out <- nloptr(initial.parameters, f, eval_grad_f=function(x) {gradient(f, x)},\n",
    "                       lb=theta.lb,ub=theta.ub,\n",
    "                       opts=list(\"algorithm\"=\"NLOPT_LD_LBFGS\",\"xtol_rel\"=1.0e-8))\n",
    "  theta <- nloptr.out$solution\n",
    "\n",
    "  log.likelihood <- log.likelihood.wrapper(theta, y.data, x.data, stage, lambda.g, NA, xi.00, P.00)$ll.cum\n",
    "  \n",
    "  # Get state vectors (xi.tt, xi.ttm1, xi.tT, P.tt, P.ttm1, P.tT) via Kalman filter\n",
    "  states <- kalman.states.wrapper(theta, y.data, x.data, stage, lambda.g, NA, xi.00, P.00)\n",
    "\n",
    "  # Two-sided (smoothed) estimates\n",
    "  trend.smoothed      <- states$smoothed$xi.tt[,4] * 4\n",
    "  potential.smoothed  <- c(states$smoothed$xi.tT[1, 3:2], states$smoothed$xi.tT[,1])\n",
    "  output.gap.smoothed <- 100 * log.output[3:(T+4)] - potential.smoothed\n",
    "\n",
    "  # Inputs for median.unbiased.estimator.stage2.R\n",
    "  y <- output.gap.smoothed[3:length(output.gap.smoothed)]\n",
    "  x <- cbind(output.gap.smoothed[2:(length(output.gap.smoothed)-1)],\n",
    "             output.gap.smoothed[1:(length(output.gap.smoothed)-2)],\n",
    "             (x.data[,3]+x.data[,4])/2,\n",
    "             states$smoothed$xi.tT[,4],\n",
    "             rep(1,T))\n",
    "\n",
    "  # One-sided (filtered) estimates  \n",
    "  trend.filtered      <- states$filtered$xi.tt[,4] * 4\n",
    "  potential.filtered  <- states$filtered$xi.tt[,1]/100\n",
    "  output.gap.filtered <- y.data[,1] - (potential.filtered * 100)\n",
    "  \n",
    "  # Save variables to return\n",
    "  return.list <- list()\n",
    "  return.list$y              <- y\n",
    "  return.list$x              <- x\n",
    "  return.list$theta          <- theta\n",
    "  return.list$log.likelihood <- log.likelihood\n",
    "  return.list$states         <- states\n",
    "  return.list$xi.00          <- xi.00\n",
    "  return.list$P.00           <- P.00\n",
    "  return.list$trend.filtered      <- trend.filtered\n",
    "  return.list$potential.filtered  <- potential.filtered\n",
    "  return.list$output.gap.filtered <- output.gap.filtered\n",
    "  return.list$trend.smoothed      <- trend.smoothed\n",
    "  return.list$potential.smoothed  <- potential.smoothed\n",
    "  return.list$output.gap.smoothed <- output.gap.smoothed\n",
    "  return(return.list)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *rstar.stage3.R*\n",
    "This function runs the model in the third stage of the HLW estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rstar.stage3 <- function(log.output,\n",
    "                         inflation,\n",
    "                         real.interest.rate,\n",
    "                         nominal.interest.rate,\n",
    "                         lambda.g,\n",
    "                         lambda.z,\n",
    "                         a3.constraint=NA,\n",
    "                         b2.constraint=NA,\n",
    "                         run.se = TRUE) {\n",
    "\n",
    "  stage <- 3\n",
    "  \n",
    "  # Data must start 4 quarters before the estimation period\n",
    "  T <- length(log.output) - 4\n",
    "\n",
    "  # Original output gap estimate\n",
    "  x.og <- cbind(rep(1,T+4), 1:(T+4))\n",
    "  y.og <- log.output\n",
    "  output.gap <- (y.og - x.og %*% solve(t(x.og) %*% x.og, t(x.og) %*% y.og)) * 100\n",
    "\n",
    "  # Initialization of state vector for Kalman filter using HP trend of log output\n",
    "  log.output.hp.trend <- hpfilter(log.output,freq=36000,type=\"lambda\",drift=FALSE)$trend\n",
    "  g.pot <- log.output.hp.trend[(g.pot.start.index):length(log.output.hp.trend)]\n",
    "  g.pot.diff <- diff(g.pot)\n",
    "  xi.00 <- c(100*g.pot[3:1],100*g.pot.diff[2:1],0,0)\n",
    "\n",
    "  # IS curve\n",
    "  y.is <- output.gap[5:(T+4)]\n",
    "  x.is <- cbind(output.gap[4:(T+3)], output.gap[3:(T+2)],\n",
    "                (real.interest.rate[4:(T+3)] + real.interest.rate[3:(T+2)])/2,\n",
    "                rep(1,T))\n",
    "  b.is <- solve(t(x.is) %*% x.is, t(x.is) %*% y.is)\n",
    "  r.is <- as.vector(y.is - x.is %*% b.is)\n",
    "  s.is <- sqrt(sum(r.is^2) / (length(r.is)-(dim(x.is)[2])))\n",
    "\n",
    "  # Phillips curve\n",
    "  y.ph <- inflation[5:(T+4)]\n",
    "  x.ph <- cbind(inflation[4:(T+3)],\n",
    "                (inflation[3:(T+2)]+inflation[2:(T+1)]+inflation[1:T])/3,\n",
    "                output.gap[4:(T+3)])\n",
    "  b.ph <- solve(t(x.ph) %*% x.ph, t(x.ph) %*% y.ph)\n",
    "  r.ph <- y.ph - x.ph %*% b.ph\n",
    "  s.ph <- sqrt(sum(r.ph^2) / (length(r.ph)-(dim(x.ph)[2])))\n",
    "  \n",
    "  y.data <- cbind(100 * log.output[5:(T+4)],\n",
    "                  inflation[5:(T+4)])\n",
    "  x.data <- cbind(100 * log.output[4:(T+3)],\n",
    "                  100 * log.output[3:(T+2)],\n",
    "                  real.interest.rate[4:(T+3)],\n",
    "                  real.interest.rate[3:(T+2)],                  \n",
    "                  inflation[4:(T+3)],\n",
    "                  (inflation[3:(T+2)]+inflation[2:(T+1)]+inflation[1:T])/3)\n",
    "\n",
    "  # Starting values for the parameter vector  \n",
    "  initial.parameters <- c(b.is[1:3], b.ph[1], b.ph[3], s.is, s.ph, 0.7) \n",
    "\n",
    "  # Set an upper and lower bound on the parameter vectors:\n",
    "  # The vector is unbounded unless values are otherwise specified  \n",
    "  theta.lb <- c(rep(-Inf,length(initial.parameters)))\n",
    "  theta.ub <- c(rep(Inf,length(initial.parameters)))\n",
    "\n",
    "  # Set a lower bound for the Phillips curve slope (b_2) of b2.constraint, if not NA\n",
    "  # In HLW, b2.constraint = 0.025  \n",
    "  if (!is.na(b2.constraint)) {\n",
    "      print(paste0(\"Setting a lower bound on b_2 of \",as.character(b2.constraint)))\n",
    "      if (initial.parameters[5] < b2.constraint) {\n",
    "          initial.parameters[5] <- b2.constraint\n",
    "      }\n",
    "      theta.lb[5] <- b2.constraint\n",
    "  }\n",
    "\n",
    "  # Set an upper bound for the IS curve slope (a_3) of a3.constraint, if not NA\n",
    "  # In HLW, a3.constraint = -0.0025  \n",
    "  if (!is.na(a3.constraint)) {\n",
    "      print(paste0(\"Setting an upper bound on a_3 of \",as.character(a3.constraint)))\n",
    "      if (initial.parameters[3] > a3.constraint) {\n",
    "          initial.parameters[3] <- a3.constraint\n",
    "      }\n",
    "      theta.ub[3] <- a3.constraint      \n",
    "  }\n",
    "\n",
    "  # Set the initial covariance matrix (see footnote 6)   \n",
    "  P.00 <- calculate.covariance(initial.parameters, theta.lb, theta.ub, y.data, x.data, stage, lambda.g, lambda.z, xi.00)\n",
    "  \n",
    "  # Get parameter estimates via maximum likelihood\n",
    "  f <- function(theta) {return(-log.likelihood.wrapper(theta, y.data, x.data, stage, lambda.g, lambda.z, xi.00, P.00)$ll.cum)}\n",
    "  nloptr.out <- nloptr(initial.parameters, f, eval_grad_f=function(x) {gradient(f, x)},\n",
    "                       lb=theta.lb,ub=theta.ub,\n",
    "                       opts=list(\"algorithm\"=\"NLOPT_LD_LBFGS\",\"xtol_rel\"=1.0e-8))\n",
    "  theta <- nloptr.out$solution\n",
    "  \n",
    "  log.likelihood <- log.likelihood.wrapper(theta, y.data, x.data, stage, lambda.g, lambda.z, xi.00, P.00)$ll.cum\n",
    "\n",
    "  # Get state vectors (xi.tt, xi.ttm1, xi.tT, P.tt, P.ttm1, P.tT) via Kalman filter\n",
    "  states <- kalman.states.wrapper(theta, y.data, x.data, stage, lambda.g, lambda.z, xi.00, P.00)\n",
    "\n",
    "  # If run.se = TRUE, compute standard errors for estimates of the states (see footnote 7) and report run time\n",
    "  if (run.se) {\n",
    "      ptm <- proc.time()\n",
    "      se <- kalman.standard.errors(T, states, theta, y.data, x.data, stage, lambda.g, lambda.z, xi.00, P.00, \n",
    "                                   niter, a3.constraint, b2.constraint)\n",
    "      print(\"Standard error procedure run time\")\n",
    "      print(proc.time() - ptm)\n",
    "  }\n",
    "  \n",
    "  # One-sided (filtered) estimates\n",
    "  trend.filtered      <- states$filtered$xi.tt[,4] * 4\n",
    "  z.filtered          <- states$filtered$xi.tt[,6]\n",
    "  rstar.filtered      <- trend.filtered + z.filtered\n",
    "  potential.filtered  <- states$filtered$xi.tt[,1]/100\n",
    "  output.gap.filtered <- y.data[,1] - (potential.filtered * 100)\n",
    "\n",
    "  # Two-sided (smoothed) estimates\n",
    "  trend.smoothed      <- states$smoothed$xi.tt[,4] * 4\n",
    "  z.smoothed          <- states$smoothed$xi.tt[,6]\n",
    "  rstar.smoothed      <- trend.smoothed + z.smoothed\n",
    "  potential.smoothed  <- states$smoothed$xi.tt[,1]/100\n",
    "  output.gap.smoothed <- y.data[,1] - (potential.smoothed * 100)\n",
    "   \n",
    "  # Save variables to return\n",
    "  return.list <- list()\n",
    "  return.list$rstar.filtered      <- rstar.filtered\n",
    "  return.list$trend.filtered      <- trend.filtered\n",
    "  return.list$z.filtered          <- z.filtered\n",
    "  return.list$potential.filtered  <- potential.filtered\n",
    "  return.list$output.gap.filtered <- output.gap.filtered\n",
    "  return.list$rstar.smoothed      <- rstar.smoothed\n",
    "  return.list$trend.smoothed      <- trend.smoothed\n",
    "  return.list$z.smoothed          <- z.smoothed\n",
    "  return.list$potential.smoothed  <- potential.smoothed\n",
    "  return.list$output.gap.smoothed <- output.gap.smoothed\n",
    "  return.list$theta               <- theta\n",
    "  return.list$log.likelihood      <- log.likelihood\n",
    "  return.list$states              <- states\n",
    "  return.list$xi.00               <- xi.00\n",
    "  return.list$P.00                <- P.00\n",
    "  return.list$y.data              <- y.data\n",
    "  return.list$initial.parameters  <- initial.parameters\n",
    "  if (run.se) { return.list$se    <- se }\n",
    "  return(return.list)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R Programs for Median Unbiased Estimators\n",
    "\n",
    "### *median.unbiased.estimator.stage1.R*\n",
    "This function computes the exponential Wald statistic of Andrews and Ploberger (1994) for a structural break with unknown break date from the first difference of the preliminary estimate of the natural rate of output from the stage 1 model to obtain the median unbiased estimate of $\\lambda_g$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "median.unbiased.estimator.stage1 <- function(series) {\n",
    "    T <- length(series)\n",
    "    y <- 400 * diff(series)\n",
    "\n",
    "    stat <- rep(T-2*4)\n",
    "    for (i in 4:(T-5)) {\n",
    "      xr <- cbind(rep(1, T-1), c(rep(0,i),rep(1,T-i-1)))\n",
    "      xi <- solve(t(xr) %*% xr)\n",
    "      b  <- solve(t(xr) %*% xr, t(xr) %*% y)\n",
    "      s3 <- sum((y-xr%*%b)^2)/(T-2-1)\n",
    "      stat[i+1-4] = b[2]/sqrt(s3*xi[2,2])\n",
    "    }\n",
    "\n",
    "    ew <- 0\n",
    "    for (i in 1:length(stat)) {\n",
    "        ew <- ew+exp(stat[i]^2/2)\n",
    "    }\n",
    "    ew  <- log(ew/length(stat))\n",
    "    mw  <- sum(stat^2) / length(stat)\n",
    "    qlr <- max(stat^2)\n",
    "\n",
    "    # Values are from Table 3 in Stock and Watson (1998)\n",
    "    # Test Statistic: Exponential Wald (EW)\n",
    "    valew <- c(0.426, 0.476, 0.516, 0.661, 0.826, 1.111,\n",
    "               1.419, 1.762, 2.355, 2.91,  3.413, 3.868, 4.925,\n",
    "               5.684, 6.670, 7.690, 8.477, 9.191, 10.693, 12.024,\n",
    "               13.089, 14.440, 16.191, 17.332, 18.699, 20.464,\n",
    "               21.667, 23.851, 25.538, 26.762, 27.874)\n",
    "    # Test Statistic: Mean Wald (MW)\n",
    "    valmw <- c(0.689, 0.757, 0.806, 1.015, 1.234, 1.632,\n",
    "               2.018, 2.390, 3.081, 3.699, 4.222, 4.776, 5.767,\n",
    "               6.586, 7.703, 8.683, 9.467, 10.101, 11.639, 13.039,\n",
    "               13.900, 15.214, 16.806, 18.330, 19.020, 20.562,\n",
    "               21.837, 24.350, 26.248, 27.089, 27.758)\n",
    "    # Test Statistic: QLR\n",
    "    valql <- c(3.198, 3.416, 3.594, 4.106, 4.848, 5.689,\n",
    "               6.682, 7.626, 9.16,  10.66, 11.841, 13.098, 15.451,\n",
    "               17.094, 19.423, 21.682, 23.342, 24.920, 28.174, 30.736,\n",
    "               33.313, 36.109, 39.673, 41.955, 45.056, 48.647, 50.983,\n",
    "               55.514, 59.278, 61.311, 64.016)\n",
    "    \n",
    "    lame <- NA\n",
    "    lamm <- NA\n",
    "    lamq <- NA\n",
    "\n",
    "    # Median-unbiased estimator of lambda_g for given values of the test\n",
    "    # statistics are obtained using the procedure described in the \n",
    "    # footnote to Stock and Watson (1998) Table 3.\n",
    "    if (ew <= valew[1]) {\n",
    "        lame <- 0\n",
    "    } else {\n",
    "        for (i in 1:(length(valew)-1)) {\n",
    "            if ((ew > valew[i]) & (ew <= valew[i+1])) {\n",
    "                lame <- i-1+(ew-valew[i])/(valew[i+1]-valew[i])\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    if (mw <= valmw[1]) {\n",
    "        lamm <- 0\n",
    "    } else {\n",
    "        for (i in 1:(length(valmw)-1)) {\n",
    "            if ((mw > valmw[i]) & (mw <= valmw[i+1])) {\n",
    "                lamm <- i-1+(mw-valmw[i])/(valmw[i+1]-valmw[i])\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    if (qlr <= valql[1]) {\n",
    "        lamq <- 0\n",
    "    } else {\n",
    "        for (i in 1:(length(valql)-1)) {\n",
    "            if ((qlr > valql[i]) & (qlr <= valql[i+1])) {\n",
    "                lamq <- i-1+(qlr-valql[i])/(valql[i+1]-valql[i])\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    if (is.na(lame) | is.na(lamm) | is.na(lamq)) {\n",
    "        print(\"At least one statistic has an NA value. \n",
    "              Check to see if your EW, MW, and/or QLR value is outside of Table 3.\")\n",
    "    }\n",
    "    \n",
    "    stats <- c(ew, mw, qlr)\n",
    "    lams  <- c(lame, lamm, lamq)\n",
    "    return(lame/(T-1))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *median.unbiased.estimator.stage2.R*\n",
    "This function applies the exponential Wald test for an intercept shift in the IS equation at an unknown date to obtain the median unbiased estimate of $\\lambda_z$, taking as input estimates from the stage 2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "median.unbiased.estimator.stage2 <- function(y, x) {\n",
    "    T <- dim(x)[1]\n",
    "    stat <- rep(0, T-2*4+1)\n",
    "    for (i in 4:(T-4)) {\n",
    "        xr <- cbind(x, c(rep(0, i), rep(1, T-i)))\n",
    "        xi <- solve(t(xr)%*%xr)\n",
    "        b <- solve(t(xr)%*%xr,t(xr)%*%y)\n",
    "        s3 <- sum((y-xr%*%b)^2)/(T-dim(xr)[2])\n",
    "        stat[i+1-4] <- b[dim(xr)[2]]/sqrt(s3*xi[dim(xr)[2],dim(xr)[2]])\n",
    "    }\n",
    "    ew <- 0\n",
    "    for (i in 1:length(stat)) {\n",
    "        ew <- ew+exp((stat[i]^2)/2)\n",
    "    }\n",
    "    ew <- log(ew/length(stat))\n",
    "    mw <- mean(stat^2)\n",
    "    qlr <- max(stat^2)\n",
    "\n",
    "    # Values are from Table 3 in Stock and Watson (1998)\n",
    "    # Test Statistic: Exponential Wald (EW)\n",
    "    valew <- c(0.426, 0.476, 0.516, 0.661, 0.826, 1.111,\n",
    "               1.419, 1.762, 2.355, 2.91,  3.413, 3.868, 4.925,\n",
    "               5.684, 6.670, 7.690, 8.477, 9.191, 10.693, 12.024,\n",
    "               13.089, 14.440, 16.191, 17.332, 18.699, 20.464,\n",
    "               21.667, 23.851, 25.538, 26.762, 27.874)\n",
    "    # Test Statistic: Mean Wald (MW)\n",
    "    valmw <- c(0.689, 0.757, 0.806, 1.015, 1.234, 1.632,\n",
    "               2.018, 2.390, 3.081, 3.699, 4.222, 4.776, 5.767,\n",
    "               6.586, 7.703, 8.683, 9.467, 10.101, 11.639, 13.039,\n",
    "               13.900, 15.214, 16.806, 18.330, 19.020, 20.562,\n",
    "               21.837, 24.350, 26.248, 27.089, 27.758)\n",
    "    # Test Statistic: QLR\n",
    "    valql <- c(3.198, 3.416, 3.594, 4.106, 4.848, 5.689,\n",
    "               6.682, 7.626, 9.16,  10.66, 11.841, 13.098, 15.451,\n",
    "               17.094, 19.423, 21.682, 23.342, 24.920, 28.174, 30.736,\n",
    "               33.313, 36.109, 39.673, 41.955, 45.056, 48.647, 50.983,\n",
    "               55.514, 59.278, 61.311, 64.016)\n",
    "    \n",
    "    lame <- NA\n",
    "    lamm <- NA\n",
    "    lamq <- NA\n",
    "    \n",
    "    # Median-unbiased estimator of lambda_g for given values of the test\n",
    "    # statistics are obtained using the procedure described in the \n",
    "    # footnote to Stock and Watson (1998) Table 3.\n",
    "    if (ew <= valew[1]) {\n",
    "        lame <- 0\n",
    "    } else {\n",
    "        for (i in 1:(length(valew)-1)) {\n",
    "            if ((ew > valew[i]) & (ew <= valew[i+1])) {\n",
    "                lame <- i-1+(ew-valew[i])/(valew[i+1]-valew[i])\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if (mw <= valmw[1]) {\n",
    "        lamm <- 0\n",
    "    } else {\n",
    "        for (i in 1:(length(valmw)-1)) {\n",
    "            if ((mw > valmw[i]) & (mw <= valmw[i+1])) {\n",
    "                lamm <- i-1+(mw-valmw[i])/(valmw[i+1]-valmw[i])\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if (qlr <= valql[1]) {\n",
    "        lamq <- 0\n",
    "    } else {\n",
    "        for (i in 1:(length(valql)-1)) {\n",
    "            if ((qlr > valql[i]) & (qlr <= valql[i+1])) {\n",
    "                lamq <- i-1+(qlr-valql[i])/(valql[i+1]-valql[i])\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    if (is.na(lame) | is.na(lamm) | is.na(lamq)) {\n",
    "            print(\"At least one statistic has an NA value. \n",
    "                   Check to see if your EW, MW, and/or QLR value is outside of Table 3.\")\n",
    "    }\n",
    "    \n",
    "    stats <- c(ew,mw,qlr)\n",
    "    lams  <- c(lame,lamm,lamq)\n",
    "    return(lame/T)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kalman Filter Programs\n",
    "\n",
    "### *kalman.states.R*\n",
    "The function kalman.states() calls the functions kalman.states.filtered() and kalman.states.smoothed() to apply the Kalman filter and smoother. It takes as input the coefficient matrices for the given state-space model as well as the conditional expectation and covariance matrix of the initial state, xi.tm1tm1 $\\left( \\xi_{t-1|t-1} \\right)$ and P.tm1tm1 $ \\left( P_{t-1|t-1} \\right)$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kalman.states <- function(xi.tm1tm1, P.tm1tm1, F, Q, A, H, R, y, x) {\n",
    "  filtered <- kalman.states.filtered(xi.tm1tm1, P.tm1tm1, F, Q, A, H, R, y, x)\n",
    "  smoothed <- kalman.states.smoothed(filtered$xi.ttm1, filtered$P.ttm1, filtered$xi.tt, filtered$P.tt,\n",
    "                                     F, Q, A, H, R, y, x)\n",
    "  return(list(\"filtered\"=filtered, \"smoothed\"=smoothed))\n",
    "}\n",
    "kalman.states.filtered <- function(xi.tm1tm1, P.tm1tm1, F, Q, A, H, R, y, x, t=1) {\n",
    "  xi.ttm1 <- as.vector(F %*% xi.tm1tm1)\n",
    "  P.ttm1 <- F %*% P.tm1tm1 %*% t(F) + Q\n",
    "  prediction.error <- (as.vector(y[t,]) - as.vector(t(A) %*% as.vector(x[t,])) - as.vector(t(H) %*% xi.ttm1))\n",
    "  HPHR <- t(H) %*% P.ttm1 %*% H + R\n",
    "  xi.tt <- xi.ttm1 + as.vector(P.ttm1 %*% H %*% solve(HPHR, prediction.error))\n",
    "  P.tt <- P.ttm1 - P.ttm1 %*% H %*% solve(HPHR, t(H) %*% P.ttm1)\n",
    "  if (t == dim(y)[1]) {\n",
    "      return(list(\"xi.ttm1\"=xi.ttm1, \"P.ttm1\"=P.ttm1, \"xi.tt\"=xi.tt, \"P.tt\"=P.tt))\n",
    "  } else {\n",
    "      tmp <- kalman.states.filtered(xi.tt, P.tt, F, Q, A, H, R, y, x, t+1)\n",
    "      return(list(\"xi.ttm1\"=rbind(xi.ttm1, tmp$xi.ttm1),\n",
    "                  \"P.ttm1\"=rbind(P.ttm1, tmp$P.ttm1),\n",
    "                  \"xi.tt\"=rbind(xi.tt, tmp$xi.tt),\n",
    "                  \"P.tt\"=rbind(P.tt, tmp$P.tt)))\n",
    "  }\n",
    "}\n",
    "kalman.states.smoothed <- function(xi.ttm1.array, P.ttm1.array, xi.tt.array, P.tt.array,\n",
    "                                   F, Q, A, H, R, y, x, t=dim(y)[1], xi.tp1T=NA, P.tp1T=NA) {\n",
    "  n <- dim(xi.ttm1.array)[2]\n",
    "  if (t == dim(y)[1]) {\n",
    "    xi.tT <- xi.tt.array[t,]\n",
    "    P.tT <- P.tt.array[((t-1)*n+1):(t*n),]\n",
    "    tmp <- kalman.states.smoothed(xi.ttm1.array, P.ttm1.array, xi.tt.array, P.tt.array,\n",
    "                                  F, Q, A, H, R, y, x, t-1, xi.tT, P.tT)\n",
    "    return(list(\"xi.tT\"=rbind(tmp$xi.tT, xi.tT),\n",
    "                \"P.tT\" =rbind(tmp$P.tT, P.tT)))\n",
    "  } else {\n",
    "    P.tt <- P.tt.array[((t-1)*n+1):(t*n),]\n",
    "    P.tp1t <- P.ttm1.array[(t*n+1):((t+1)*n),]\n",
    "    J.t <- P.tt %*% t(F) %*% solve(P.tp1t)\n",
    "    xi.tt <- xi.tt.array[t,]\n",
    "    xi.tp1t <- xi.ttm1.array[t+1,]\n",
    "    xi.tT <- xi.tt + as.vector(J.t %*% (xi.tp1T - xi.tp1t))\n",
    "    P.tT <- P.tt + J.t %*% (P.tp1T - P.tp1t) %*% t(J.t)\n",
    "    if (t > 1) {\n",
    "      tmp <- kalman.states.smoothed(xi.ttm1.array, P.ttm1.array, xi.tt.array, P.tt.array,\n",
    "                                    F, Q, A, H, R, y, x, t-1, xi.tT, P.tT)\n",
    "      return(list(\"xi.tT\"=rbind(tmp$xi.tT, xi.tT),\n",
    "                  \"P.tT\" =rbind(tmp$P.tT, P.tT)))\n",
    "    } else {\n",
    "      return(list(\"xi.tT\"=xi.tT, \"P.tT\"=P.tT))\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *kalman.states.wrapper.R*\n",
    "This is a wrapper function for *kalman.states.R* that specifies inputs based on the estimation stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kalman.states.wrapper <- function(parameters, y.data, x.data, stage = NA,\n",
    "                                  lambda.g=NA, lambda.z=NA, xi.00=NA, P.00=NA){\n",
    "\n",
    "    if (stage == 1) {\n",
    "        out <- unpack.parameters.stage1(parameters, y.data, x.data,\n",
    "                                        xi.00, P.00)\n",
    "    } else if (stage == 2) {\n",
    "        out <- unpack.parameters.stage2(parameters, y.data, x.data,\n",
    "                                        lambda.g, xi.00, P.00)\n",
    "    } else if (stage == 3) {\n",
    "        out <- unpack.parameters.stage3(parameters, y.data, x.data,\n",
    "                                        lambda.g, lambda.z, xi.00, P.00)\n",
    "    } else {\n",
    "        stop('You need to enter a stage number in kalman.states.wrapper.')\n",
    "    }\n",
    "\n",
    "  for (n in names(out)) {\n",
    "      eval(parse(text=paste0(n, \"<-out$\", n)))\n",
    "  }\n",
    "  T <- dim(y.data)[1]\n",
    "  states <- kalman.states(xi.00, P.00, F, Q, A, H, R, y.data, x.data)\n",
    "  if (stage == 1) {\n",
    "      states$filtered$xi.tt <- states$filtered$xi.tt + cbind(1:T,0:(T-1),-1:(T-2)) * parameters[5]\n",
    "      states$smoothed$xi.tT <- states$smoothed$xi.tT + cbind(1:T,0:(T-1),-1:(T-2)) * parameters[5]\n",
    "  }\n",
    "return(states)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Likelihood Programs\n",
    "\n",
    "### *kalman.log.likelihood.R*\n",
    "This function takes as input the coefficient matrices of the given state-space model and the conditional expectation and covariance matrix of the initial state and returns the log likelihood value and a vector with the log likelihood at each time $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kalman.log.likelihood <- function(xi.tm1tm1, P.tm1tm1, F, Q, A, H, R, y, x) {\n",
    "    T <- dim(y)[1]\n",
    "    n <- dim(y)[2]\n",
    "    ll.vec <- matrix(NA,T,1)\n",
    "    ll.cum <- 0\n",
    "    xi.tt <- xi.tm1tm1\n",
    "    P.tt  <- P.tm1tm1    \n",
    "    for (t in 1:T){\n",
    "\n",
    "        xi.ttm1 <- F %*% xi.tt\n",
    "        P.ttm1 <- F %*% P.tt %*% t(F) + Q\n",
    "        prediction.error <- (as.vector(y[t,]) - as.vector(t(A) %*% as.vector(x[t,])) - as.vector(t(H) %*% xi.ttm1))\n",
    "        HPHR <- t(H) %*% P.ttm1 %*% H + R\n",
    "        ll.vec[t] <- drop(-(n / 2) * log(2 * atan(1) * 4) - 0.5 * log(det(HPHR))\n",
    "                          -0.5 * prediction.error %*% solve(HPHR, prediction.error))\n",
    "        ll.cum <- ll.cum + ll.vec[t]\n",
    "\n",
    "        xi.tt <- xi.ttm1 + P.ttm1 %*% H %*% solve(HPHR, prediction.error)\n",
    "        P.tt  <- P.ttm1 - P.ttm1 %*% H %*% solve(HPHR, t(H) %*% P.ttm1)\n",
    "    }\n",
    "    return(list(\"ll.vec\"=ll.vec,\"ll.cum\"=ll.cum))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *log.likelihood.wrapper.R*\n",
    "This is a wrapper function for *kalman.log.likelihood.R* that specifies inputs based on the estimation stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log.likelihood.wrapper <- function(parameters, y.data, x.data, stage = NA,\n",
    "                                   lambda.g=NA, lambda.z=NA, xi.00=NA, P.00=NA){\n",
    "\n",
    "    if (stage == 1) {\n",
    "        out <- unpack.parameters.stage1(parameters, y.data, x.data,\n",
    "                                        xi.00, P.00)\n",
    "    } else if (stage == 2) {\n",
    "        out <- unpack.parameters.stage2(parameters, y.data, x.data,\n",
    "                                        lambda.g, xi.00, P.00)\n",
    "    } else if (stage == 3) {\n",
    "        out <- unpack.parameters.stage3(parameters, y.data, x.data,\n",
    "                                        lambda.g, lambda.z, xi.00, P.00)\n",
    "    } else {\n",
    "        stop('You need to enter a stage number in log.likelihood.wrapper.')\n",
    "    }\n",
    "\n",
    "\n",
    "  for (n in names(out)) {\n",
    "      eval(parse(text=paste0(n, \"<-out$\", n)))\n",
    "  }\n",
    "  return(kalman.log.likelihood(xi.00, P.00, F, Q, A, H, R, y.data, x.data))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### *kalman.standard.errors.R*\n",
    "This function computes confidence intervals and corresponding standard errors for the estimates of the states using Hamilton's (1986) Monte Carlo procedure that accounts for both filter and parameter uncertainty. See footnote 7 in HLW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kalman.standard.errors <- function(T, states, theta, y.data, x.data, stage,\n",
    "                                   lambda.g, lambda.z, xi.00, P.00, niter = 5000,\n",
    "                                   a3.constraint=NA, b2.constraint=NA) {\n",
    "\n",
    "    print('Computing Standard Errors')\n",
    "    \n",
    "    # Set a3.constraint to -0.0025 if a constraint is not specified in stage 3\n",
    "    if (is.na(a3.constraint)) {\n",
    "        a3.constraint <- -0.0025\n",
    "    }\n",
    "    # Set b2.constraint to 0.025 if a constraint is not specified in stage 3\n",
    "    if (is.na(b2.constraint)) {\n",
    "        b2.constraint <- 0.025\n",
    "    }\n",
    "\n",
    "    print(\"Standard Error Procedure: a3.constraint\")\n",
    "    print(a3.constraint)\n",
    "    \n",
    "    print(\"Standard Error Procedure: b2.constraint\")\n",
    "    print(b2.constraint)\n",
    "\n",
    "    n.params <- length(theta)\n",
    "    n.state.vars <- length(xi.00)\n",
    "\n",
    "    # Return vector of log likelihood values at each time t \n",
    "    log.likelihood.estimated.vector <- log.likelihood.wrapper(theta, y.data, x.data, stage = 3,\n",
    "                                                              lambda.g=lambda.g, lambda.z=lambda.z, \n",
    "                                                              xi.00=xi.00, P.00=P.00)$ll.vec\n",
    "    stin <- states$smoothed$xi.tT[1,] # First smoothed state vector\n",
    "    pp1  <- states$filtered$P.ttm1[1:n.state.vars,] # First covariance matrix\n",
    "    eigenstuff.pp1   <- eigen(pp1)\n",
    "    eigenvectors.pp1 <- eigenstuff.pp1$vectors # Eigenvectors of first covariance matrix\n",
    "    # Eigenvectors without a positive first entry are multiplied by -1 to ensure\n",
    "    # consistency across different versions of R, which choose the sign differently\n",
    "    for (l in 1:n.state.vars) {\n",
    "        if (eigenvectors.pp1[1,l] < 0 ) { eigenvectors.pp1[,l] <- -eigenvectors.pp1[,l] }\n",
    "    } \n",
    "    eigenvalues.pp1  <- eigenstuff.pp1$value   # Eigenvalues of first covariance matrix\n",
    "    dg   <- diag(x = eigenvalues.pp1) \n",
    "    hh2  <- eigenvectors.pp1 %*% sqrt(dg)    \n",
    "\n",
    "    # Compute information matrix from difference in gradients of the likelihood function\n",
    "    # from varying theta (parameter vector) values\n",
    "    likelihood.gradient <- matrix(NA,T,n.params)\n",
    "    for (i in 1:n.params){\n",
    "        delta   <- max(theta[i]*1e-6, 1e-6)\n",
    "        d.theta <- theta\n",
    "        d.theta[i] <- theta[i] + delta\n",
    "        likelihood.gradient[,i] <-  (log.likelihood.wrapper(d.theta, y.data, x.data, stage = 3,\n",
    "                                                            lambda.g=lambda.g, lambda.z=lambda.z, \n",
    "                                                            xi.00=xi.00, P.00=P.00)$ll.vec - \n",
    "                                         log.likelihood.estimated.vector)/delta\n",
    "    }\n",
    "    info <- solve(t(likelihood.gradient) %*% likelihood.gradient) # Information matrix\n",
    "    bse <- sqrt(diag(info))\n",
    "    t.stats <- abs(theta) / bse\n",
    "\n",
    "    # Smoothed estimates\n",
    "    g      <- 4 * states$smoothed$xi.tT[,4]\n",
    "    ypot   <- states$smoothed$xi.tT[,1]\n",
    "    z      <- states$smoothed$xi.tT[,6]\n",
    "    rstar  <- g + z\n",
    "\n",
    "    # cum1 cumulates terms for parameter uncertainty;\n",
    "    # cum2 cumulates terms for filter uncertainty\n",
    "    cum1 <- matrix(0,T,3)\n",
    "    cum2 <- matrix(0,T,3)\n",
    "    eigenstuff.info   <- eigen(info)\n",
    "    eigenvectors.info <- eigenstuff.info$vectors # Eigenvectors of information matrix\n",
    "    # Eigenvectors without a positive first entry are multiplied by -1 to ensure\n",
    "    # consistency across different versions of R, which choose the sign differently\n",
    "    for (l in 1:n.params) {\n",
    "        if (eigenvectors.info[1,l] < 0 ) { eigenvectors.info[,l] <- -eigenvectors.info[,l] }\n",
    "    }\n",
    "    eigenvalues.info  <- eigenstuff.info$value # Eigenvalues of information matrix\n",
    "    dg <- diag(x = eigenvalues.info)\n",
    "    hh <- eigenvectors.info %*% sqrt(dg)\n",
    "\n",
    "    set.seed(50)\n",
    "\n",
    "    # Store the number of draws excluded for violating constraints\n",
    "    good.draws                 <- 0\n",
    "    excluded.draw.counter      <- 0\n",
    "    excluded.draw.counter.a3   <- 0\n",
    "    excluded.draw.counter.b2   <- 0\n",
    "    excluded.draw.counter.a1a2 <- 0\n",
    "\n",
    "    # See HLW footnote 7 for description of procedure\n",
    "    # niter is the number of iterations; we discard draws that violate constraints\n",
    "    while (good.draws < niter) {\n",
    "      theta.i <- (hh %*% rnorm(n.params) + theta)[,1] \n",
    "      if ( (theta.i[3] <= a3.constraint) & (theta.i[5] >= b2.constraint) & (theta.i[1] + theta.i[2] < 1) ) {\n",
    "          xi.00.i  <- c(t(hh2 %*% rnorm(n.state.vars) + stin)) \n",
    "          states.i <- kalman.states.wrapper(theta.i, y.data, x.data, stage, lambda.g, lambda.z, xi.00.i, pp1)\n",
    "\n",
    "          g.i    <- 4 * states.i$smoothed$xi.tT[,4]\n",
    "          ypot.i <- states.i$smoothed$xi.tT[,1]\n",
    "          z.i    <- states.i$smoothed$xi.tT[,6]\n",
    "          r.i    <- g.i + z.i          \n",
    "                    \n",
    "          cum1[,1] <- cum1[,1]+(ypot.i-ypot)^2\n",
    "          cum1[,2] <- cum1[,2]+(r.i-rstar)^2\n",
    "          cum1[,3] <- cum1[,3]+(g.i-g)^2\n",
    "\n",
    "          P.ttm1.i   <- states.i$smoothed$P.tT\n",
    "          P.ttm1.i.f <- states.i$filtered$P.tt\n",
    "          for (j in 1:(T-1)){\n",
    "              cum2[j,1]  <- cum2[j,1] + P.ttm1.i[(j * n.state.vars +1),1]\n",
    "              cum2[j,2]  <- cum2[j,2] + 16 * P.ttm1.i[(j*n.state.vars+4),4] + P.ttm1.i[(j*n.state.vars+6),6]\n",
    "              cum2[j,3]  <- cum2[j,3] + P.ttm1.i[(j*n.state.vars+4),4]\n",
    "          }\n",
    "          cum2[T,1] <- cum2[T,1] + P.ttm1.i.f[((T-1)*n.state.vars+1),1]\n",
    "          cum2[T,2] <- cum2[T,2] + (16 * P.ttm1.i.f[((T-1)*n.state.vars+4),4] + P.ttm1.i.f[((T-1)*n.state.vars+6),6])\n",
    "          cum2[T,3] <- cum2[T,3] + P.ttm1.i.f[((T-1)*n.state.vars+4),4]\n",
    "          good.draws <- good.draws + 1\n",
    "      } else {\n",
    "          excluded.draw.counter <- excluded.draw.counter + 1\n",
    "          if (theta.i[3] > a3.constraint) {\n",
    "              excluded.draw.counter.a3 <- excluded.draw.counter.a3 + 1\n",
    "          }\n",
    "          if (theta.i[5] < b2.constraint) {\n",
    "              excluded.draw.counter.b2 <- excluded.draw.counter.b2 + 1\n",
    "          }\n",
    "          if ((theta.i[1] + theta.i[2]) >= 1) {\n",
    "              excluded.draw.counter.a1a2 <- excluded.draw.counter.a1a2 + 1\n",
    "          }\n",
    "      }\n",
    "    }\n",
    "    cum1 <- cum1/niter # Measure of parameter uncertainty\n",
    "    cum2 <- cum2/niter # Measure of filter uncertainty\n",
    "    cum2[,3] <- 16*cum2[,3] # Variance for growth at an annualized rate\n",
    "\n",
    "    # Standard errors for estimates of the states\n",
    "    # Order: y*, r*, g\n",
    "    se <- sqrt(cum1 + cum2)\n",
    "\n",
    "    rm(.Random.seed)\n",
    "    return(list(\"se.mean\"=colMeans(se),\n",
    "                \"se\"=se,\"t.stats\"=t.stats,\"bse\"=bse,\n",
    "                \"number.excluded\"=excluded.draw.counter,\n",
    "                \"number.excluded.a3\"=excluded.draw.counter.a3,\n",
    "                \"number.excluded.b2\"=excluded.draw.counter.b2,\n",
    "                \"number.excluded.a1a2\"=excluded.draw.counter.a1a2))\n",
    "}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *calculate.covariance.R*\n",
    "This function calculates the covariance matrix of the initial state from the gradients of the likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "calculate.covariance <- function(initial.parameters,theta.lb,theta.ub,y.data,x.data,\n",
    "                                 stage,lambda.g=NA,lambda.z=NA,xi.00) {\n",
    "\n",
    "  n.state.vars <- length(xi.00)\n",
    "\n",
    "  # Set covariance matrix equal to 0.2 times the identity matrix\n",
    "  P.00 <- diag(0.2,n.state.vars,n.state.vars)\n",
    "  \n",
    "  # Get parameter estimates via maximum likelihood\n",
    "  f <- function(theta) {return(-log.likelihood.wrapper(theta, y.data, x.data, stage, lambda.g, lambda.z, xi.00, P.00)$ll.cum)}\n",
    "  nloptr.out <- nloptr(initial.parameters, f, eval_grad_f=function(x) {gradient(f, x)},\n",
    "                       lb=theta.lb,ub=theta.ub,opts=list(\"algorithm\"=\"NLOPT_LD_LBFGS\",\"xtol_rel\"=1.0e-8))\n",
    "  theta <- nloptr.out$solution\n",
    "  \n",
    "  # Run Kalman filter with above covariance matrix and corresponding parameter estimates\n",
    "  states <- kalman.states.wrapper(theta, y.data, x.data, stage, lambda.g, lambda.z, xi.00, P.00)\n",
    "\n",
    "  # Save initial covariance matrix \n",
    "  P.00 <- states$filtered$P.ttm1[1:n.state.vars,]\n",
    "  \n",
    "return(P.00)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *format.output.R*\n",
    "This function generates a dataframe to be written to a CSV containing one-sided estimates, parameter values, standard errors, and other statistics of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "format.output <- function(country.estimation, one.sided.est.country, real.rate.country, start, end, run.se = TRUE) {\n",
    "    output.country <- data.frame(matrix(NA,dim(one.sided.est.country)[1],22))\n",
    "    \n",
    "    output.country[,1]   <- seq(from = (as.Date(ti(shiftQuarter(start,-1),'quarterly'))+1), \n",
    "                                to = (as.Date(ti(shiftQuarter(end,-1),tif='quarterly'))+1), by = 'quarter')\n",
    "    output.country[,2:5] <- one.sided.est.country\n",
    "\n",
    "    output.country[1,7]    <- \"Parameter Point Estimates\"\n",
    "    output.country[2,7:15] <- c(\"a_1\",\"a_2\",\"a_3\",\"b_1\",\"b_2\",\"sigma_1\",\"sigma_2\",\"sigma_4\",\"a_1 + a_2\")\n",
    "    output.country[3,7:14] <- country.estimation$out.stage3$theta\n",
    "    output.country[3,15]   <- country.estimation$out.stage3$theta[1] + country.estimation$out.stage3$theta[2]\n",
    "    # Include standard errors in output only if run.se switch is TRUE\n",
    "    if (run.se) {\n",
    "        output.country[4,7]    <- \"T Statistics\"\n",
    "        output.country[5,7:14] <- country.estimation$out.stage3$se$t.stats\n",
    "        \n",
    "        output.country[8,7]    <- \"Average Standard Errors\"\n",
    "        output.country[9,7:9]  <- c(\"y*\",\"r*\",\"g\")\n",
    "        output.country[10,7:9] <- country.estimation$out.stage3$se$se.mean\n",
    "        \n",
    "        output.country[12,7]   <- \"Restrictions on MC draws: a_3 < -0.0025; b_2 > 0.025; a_1 + a_2 < 1\"\n",
    "        output.country[13,7]   <- \"Draws excluded:\"\n",
    "        output.country[13,9]   <- country.estimation$out.stage3$se$number.excluded\n",
    "        output.country[13,10]  <- \"Total:\"; output.country[13,11] <- niter\n",
    "        output.country[14,7]   <- \"Percent excluded:\"\n",
    "        output.country[14,9]   <- as.numeric(output.country[13,9]) / \n",
    "                                 (as.numeric(output.country[13,9]) + as.numeric(output.country[13,11]))\n",
    "        output.country[15,7]   <- \"Draws excluded because a_3 > -0.0025:\"\n",
    "        output.country[15,11]  <- country.estimation$out.stage3$se$number.excluded.a3\n",
    "        output.country[16,7]   <- \"Draws excluded because b_2 <  0.025:\"\n",
    "        output.country[16,11]  <- country.estimation$out.stage3$se$number.excluded.b2\n",
    "        output.country[17,7]   <- \"Draws excluded because a_1 + a_2 < 1:\"\n",
    "        output.country[17,11]  <- country.estimation$out.stage3$se$number.excluded.a1a2\n",
    "    }\n",
    "    \n",
    "    output.country[19,7]  <- \"Signal-to-noise Ratios\"\n",
    "    output.country[20,7]  <- \"lambda_g\"; output.country[20,8] <- country.estimation$lambda.g\n",
    "    output.country[21,7]  <- \"lambda_z\"; output.country[21,8] <- country.estimation$lambda.z\n",
    "    output.country[19,11] <- \"Log Likelihood\"; output.country[20,11] <- country.estimation$out.stage3$log.likelihood\n",
    "\n",
    "    output.country[24,7]    <- \"State vector: [y_{t}* y_{t-1}* y_{t-2}* g_{t-1} g_{t-2} z_{t-1} z_{t-2}]\"\n",
    "    output.country[25,7]    <- \"Initial State Vector\"\n",
    "    output.country[26,7:13] <- country.estimation$out.stage3$xi.00\n",
    "    output.country[28,7]    <- \"Initial Covariance Matrix\"\n",
    "    output.country[29:35,7:13] <- country.estimation$out.stage3$P.00\n",
    "\n",
    "    if (run.se) {\n",
    "        output.country[,17]    <- seq(from = (as.Date(ti(shiftQuarter(start,-1),'quarterly'))+1), \n",
    "                                      to = (as.Date(ti(shiftQuarter(end,-1),tif='quarterly'))+1), by = 'quarter')\n",
    "        output.country[,18:20] <- country.estimation$out.stage3$se$se\n",
    "    }\n",
    "    \n",
    "    output.country[,22] <- real.rate.country[5:length(real.rate.country)] - \n",
    "                            country.estimation$out.stage3$rstar.filtered\n",
    "\n",
    "return(output.country)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Estimation for Each Economy\n",
    "\n",
    "### United States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in output of prepare.rstar.data.us.R\n",
    "us.data <- read.table(\"inputData/rstar.data.us.csv\",\n",
    "                      sep = ',', na.strings = \".\", header=TRUE, stringsAsFactors=FALSE)\n",
    "\n",
    "us.log.output             <- us.data$gdp.log\n",
    "us.inflation              <- us.data$inflation\n",
    "us.inflation.expectations <- us.data$inflation.expectations\n",
    "us.nominal.interest.rate  <- us.data$interest\n",
    "us.real.interest.rate     <- us.nominal.interest.rate - us.inflation.expectations\n",
    "\n",
    "# Run HLW estimation for the US\n",
    "us.estimation <- run.hlw.estimation(us.log.output, us.inflation, us.real.interest.rate, us.nominal.interest.rate,\n",
    "                                    a3.constraint = a3.constraint, b2.constraint = b2.constraint, run.se = run.se)\n",
    "\n",
    "# One-sided (filtered) estimates\n",
    "one.sided.est.us <- cbind(us.estimation$out.stage3$rstar.filtered,\n",
    "                          us.estimation$out.stage3$trend.filtered,\n",
    "                          us.estimation$out.stage3$z.filtered,\n",
    "                          us.estimation$out.stage3$output.gap.filtered)\n",
    "\n",
    "# Save one-sided estimates to CSV\n",
    "write.table(one.sided.est.us, 'output/one.sided.est.us.csv', row.names = FALSE, \n",
    "            col.names = c(\"rstar\",\"g\",\"z\",\"output gap\"), quote = FALSE, sep = ',', na = \".\")\n",
    "\n",
    "# Save output to CSV\n",
    "output.us <- format.output(us.estimation, one.sided.est.us, us.real.interest.rate, sample.start, sample.end)\n",
    "write.table(output.us, 'output/output.us.csv', col.names = output.col.names, \n",
    "            quote=FALSE, row.names=FALSE, sep = ',', na = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in output of prepare.rstar.data.ca.R\n",
    "ca.data <- read.table(\"inputData/rstar.data.ca.csv\",\n",
    "                      sep = ',', na.strings = \".\", header=TRUE, stringsAsFactors=FALSE)\n",
    "\n",
    "ca.log.output             <- ca.data$gdp.log\n",
    "ca.inflation              <- ca.data$inflation\n",
    "ca.inflation.expectations <- ca.data$inflation.expectations\n",
    "ca.nominal.interest.rate  <- ca.data$interest\n",
    "ca.real.interest.rate     <- ca.nominal.interest.rate - ca.inflation.expectations\n",
    "\n",
    "# Run HLW estimation for Canada\n",
    "ca.estimation <- run.hlw.estimation(ca.log.output, ca.inflation, ca.real.interest.rate, ca.nominal.interest.rate,\n",
    "                                    a3.constraint = a3.constraint, b2.constraint = b2.constraint, run.se = run.se)\n",
    "\n",
    "# One-sided (filtered) estimates\n",
    "one.sided.est.ca <- cbind(ca.estimation$out.stage3$rstar.filtered,\n",
    "                          ca.estimation$out.stage3$trend.filtered,\n",
    "                          ca.estimation$out.stage3$z.filtered,\n",
    "                          ca.estimation$out.stage3$output.gap.filtered)\n",
    "\n",
    "# Save one-sided estimates to CSV\n",
    "write.table(one.sided.est.ca, 'output/one.sided.est.ca.csv', row.names = FALSE, \n",
    "            col.names = c(\"rstar\",\"g\",\"z\",\"output gap\"), quote = FALSE, sep = ',', na = \".\")\n",
    "\n",
    "# Save output to CSV\n",
    "output.ca <- format.output(ca.estimation, one.sided.est.ca, ca.real.interest.rate, sample.start, sample.end, run.se = run.se)\n",
    "write.table(output.ca, 'output/output.ca.csv', col.names = output.col.names, \n",
    "            quote=FALSE, row.names=FALSE, sep = ',', na = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Euro Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in output of prepare.rstar.data.ea.R\n",
    "ea.data <- read.table(\"inputData/rstar.data.ea.csv\",\n",
    "                      sep = ',', na.strings = \".\", header=TRUE, stringsAsFactors=FALSE)\n",
    "\n",
    "ea.log.output             <- ea.data$gdp.log\n",
    "ea.inflation              <- ea.data$inflation\n",
    "ea.inflation.expectations <- ea.data$inflation.expectations\n",
    "ea.nominal.interest.rate  <- ea.data$interest\n",
    "ea.real.interest.rate     <- ea.nominal.interest.rate - ea.inflation.expectations\n",
    "\n",
    "# Run HLW estimation for the Euro Area\n",
    "ea.estimation <- run.hlw.estimation(ea.log.output, ea.inflation, ea.real.interest.rate, ea.nominal.interest.rate,\n",
    "                                    a3.constraint = a3.constraint, b2.constraint = b2.constraint, run.se = run.se)\n",
    "\n",
    "# One-sided (filtered) estimates\n",
    "one.sided.est.ea <- cbind(ea.estimation$out.stage3$rstar.filtered,\n",
    "                          ea.estimation$out.stage3$trend.filtered,\n",
    "                          ea.estimation$out.stage3$z.filtered,\n",
    "                          ea.estimation$out.stage3$output.gap.filtered)\n",
    "\n",
    "# Save one-sided estimates to CSV\n",
    "write.table(one.sided.est.ea, 'output/one.sided.est.ea.csv', row.names = FALSE, \n",
    "           col.names = c(\"rstar\",\"g\",\"z\",\"output gap\"), quote = FALSE, sep = ',', na = \".\")\n",
    "\n",
    "# Save output to CSV\n",
    "output.ea <- format.output(ea.estimation, one.sided.est.ea, ea.real.interest.rate, ea.sample.start, \n",
    "                           sample.end, run.se = run.se)\n",
    "write.table(output.ea, 'output/output.ea.csv', col.names = output.col.names, \n",
    "            quote=FALSE, row.names=FALSE, sep = ',', na = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### United Kingdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in output of prepare.rstar.data.uk.R\n",
    "uk.data <- read.table(\"inputData/rstar.data.uk.csv\",\n",
    "                      sep = ',', na.strings = \".\", header=TRUE, stringsAsFactors=FALSE)\n",
    "\n",
    "\n",
    "uk.log.output             <- uk.data$gdp.log\n",
    "uk.inflation              <- uk.data$inflation\n",
    "uk.inflation.expectations <- uk.data$inflation.expectations\n",
    "uk.nominal.interest.rate  <- uk.data$interest\n",
    "uk.real.interest.rate     <- uk.nominal.interest.rate - uk.inflation.expectations\n",
    "\n",
    "# Run HLW estimation for the UK\n",
    "uk.estimation <- run.hlw.estimation(uk.log.output, uk.inflation, uk.real.interest.rate, uk.nominal.interest.rate,\n",
    "                                    a3.constraint = a3.constraint, b2.constraint = b2.constraint, run.se = run.se)\n",
    "\n",
    "# One-sided (filtered) estimates\n",
    "one.sided.est.uk <- cbind(uk.estimation$out.stage3$rstar.filtered,\n",
    "                          uk.estimation$out.stage3$trend.filtered,\n",
    "                          uk.estimation$out.stage3$z.filtered,\n",
    "                          uk.estimation$out.stage3$output.gap.filtered)\n",
    "\n",
    "# Save one-sided estimates to CSV\n",
    "write.table(one.sided.est.uk, 'output/one.sided.est.uk.csv', row.names = FALSE, \n",
    "            col.names = c(\"rstar\",\"g\",\"z\",\"output gap\"), quote = FALSE, sep = ',', na = \".\")\n",
    "\n",
    "# Save output to CSV\n",
    "output.uk <- format.output(uk.estimation, one.sided.est.uk, uk.real.interest.rate, sample.start, sample.end, run.se = run.se)\n",
    "write.table(output.uk, 'output/output.uk.csv', col.names = output.col.names, \n",
    "            quote=FALSE, row.names=FALSE, sep = ',', na = '')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
